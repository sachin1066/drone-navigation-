{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvrlab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "#dense161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace)\n",
       "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Dropout(p=0.5)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.classifier= nn.Sequential(*list(alexnet.classifier.children())[:-1])\n",
    "# (alexnet.classifier)\n",
    "alexnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.bias.data.zero_()    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Sequential(nn.BatchNorm2d(256),nn.ReLU(),nn.Conv2d(256,1024,1))\n",
    "conv1 = InitializeWeights(conv1)\n",
    "conv2 = nn.Sequential(nn.BatchNorm2d(1024),nn.ReLU(),nn.Conv2d(1024,128,2))\n",
    "conv2 = InitializeWeights(conv2)\n",
    "conv3 = nn.Sequential(nn.BatchNorm2d(128),nn.ReLU(),nn.Conv2d(128,16,1))\n",
    "conv3 = InitializeWeights(conv3)\n",
    "norm1 = nn.BatchNorm2d(16)\n",
    "norm1 = InitializeWeights(norm1)\n",
    "fc1= nn.Sequential(nn.Linear(4096, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel4(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyModel4, self).__init__()\n",
    "        self.pretrained_model = nn.Sequential(alexnet)\n",
    "#         self.conv1 = conv1\n",
    "#         self.conv2 = conv2\n",
    "#         self.conv3 = conv3\n",
    "#         self.norm1 = norm1\n",
    "        self.fc1 = fc1\n",
    "   \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.norm1(x)\n",
    "        #print(x.size())\n",
    "#         x = x.view(-1, self.num_flat_features(x))\n",
    "#         #print(x.size())\n",
    "#         #x = self.conv4(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModel4(alexnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09164738655090332\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "input=Variable(torch.randn(1,3,180,320))\n",
    "tic=time.time()\n",
    "output=net(input)\n",
    "tac=time.time()\n",
    "print(tac-tic)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "torch.Size([64, 3, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  16\n",
      "Total number of parameters --->  57007937\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in net.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(5, 3, 180, 320))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTrainData_21000_distance.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8025000095367432"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrainT[0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData_random_distance_1.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#print(xtrainT.size(), xtestT.size())\n",
    "#print(xtrainT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))\n",
    "print(torch.min(xtrainT), torch.max(xtrainT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "#xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#xtestT = torch.div(xtestT,255.0)\n",
    "#mn = [0.406,0.456,0.485]\n",
    "#sd = [0.225,0.224,0.229]\n",
    "#norm = Normalize(mn,sd)\n",
    "#xtestT = norm(xtestT)\n",
    "#print(xtestT.size(), ytestT.size())\n",
    "#print(torch.min(xtestT), torch.max(xtestT),torch.min(ytestT), torch.max(ytestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = batch_rgb_to_bgr(x)\n",
    "    x = torch.div(x,255.0)\n",
    "    mn = [0.406,0.456,0.485]\n",
    "    sd = [0.225,0.224,0.229]\n",
    "    x[:,0,:,:] = (x[:,0,:,:]-mn[0])/sd[0]\n",
    "    x[:,1,:,:] = (x[:,1,:,:]-mn[1])/sd[1]\n",
    "    x[:,2,:,:] = (x[:,2,:,:]-mn[2])/sd[2]\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff = torch.abs(tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "        loss = torch.mean(diff)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BerhuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BerhuLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        mt = tar[:,0]\n",
    "        mp = inp[:,0]\n",
    "        diff = torch.abs(mt-mp)        \n",
    "        lossval = 0.0        \n",
    "        c = 0.2 * torch.max(diff)\n",
    "        l1 = torch.mean(diff)\n",
    "        l2 = torch.mean(torch.pow(diff,2))\n",
    "        if l1 <= c:\n",
    "            lossval = l1\n",
    "        else:\n",
    "            lossval = (l2+c**2)/(2*c)\n",
    "        \n",
    "        return lossval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = torch.FloatTensor(ytrainT[5,0])\n",
    "alpha = ytrainT[5,0]\n",
    "#print(alpha.shape)\n",
    "xt = torch.FloatTensor([np.cos(alpha),np.sin(alpha)])\n",
    "print(ytrainT[5,0],xt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ytrainT[5:10,0]\n",
    "print(torch.cos(alpha[0:1]-alpha[1:2]))\n",
    "xt = torch.stack([torch.cos(alpha[0:1]),torch.sin(alpha[0:1])])\n",
    "xp = torch.stack([torch.cos(alpha[1:2]),torch.sin(alpha[1:2])])\n",
    "print(xt[0],xt[1])\n",
    "#print(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar,batch_sz):\n",
    "        alpha_t = tar[:,0]\n",
    "        alpha_p = inp[:,0]\n",
    "        #xt = torch.stack([torch.cos(alpha_t),torch.sin(alpha_t)])\n",
    "        #xp = torch.stack([torch.cos(alpha_p),torch.sin(alpha_p)])\n",
    "        #cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        #loss = cos(xt, xp)\n",
    "        #return loss\n",
    "        loss = Variable(torch.FloatTensor(batch_sz).zero_(), requires_grad=False).cuda()\n",
    "        for i in range(batch_sz):          \n",
    "            loss[i] = torch.cos(alpha_t[i:i+1]-alpha_p[i:i+1])\n",
    "            \n",
    "        lossval = 1.0-torch.mean(loss)    \n",
    "        #print(lossval)\n",
    "        return lossval\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.00929350 at epoch 1 completed in 0m 18s\n",
      "Loss 0.009394781690623077 is bigger than Loss 0.009293501877358987 in the prev epoch \n",
      "Loss = 0.00939478 at epoch 2 completed in 0m 17s\n",
      "Loss 0.009367700280355551 is bigger than Loss 0.009293501877358987 in the prev epoch \n",
      "Loss = 0.00936770 at epoch 3 completed in 0m 17s\n",
      "Loss 0.009373849005039249 is bigger than Loss 0.009293501877358987 in the prev epoch \n",
      "Loss = 0.00937385 at epoch 4 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00926310 at epoch 5 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00924198 at epoch 6 completed in 0m 17s\n",
      "Loss 0.00930853688291141 is bigger than Loss 0.009241977812988415 in the prev epoch \n",
      "Loss = 0.00930854 at epoch 7 completed in 0m 17s\n",
      "Loss 0.009298215473869017 is bigger than Loss 0.009241977812988415 in the prev epoch \n",
      "Loss = 0.00929822 at epoch 8 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00916692 at epoch 9 completed in 0m 17s\n",
      "Loss 0.009329305336411511 is bigger than Loss 0.00916691933359419 in the prev epoch \n",
      "Loss = 0.00932931 at epoch 10 completed in 0m 17s\n",
      "Loss 0.009228378813713786 is bigger than Loss 0.00916691933359419 in the prev epoch \n",
      "Loss = 0.00922838 at epoch 11 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00914717 at epoch 12 completed in 0m 17s\n",
      "Loss 0.009196281406496256 is bigger than Loss 0.009147168656012842 in the prev epoch \n",
      "Loss = 0.00919628 at epoch 13 completed in 0m 17s\n",
      "Loss 0.009165334129439935 is bigger than Loss 0.009147168656012842 in the prev epoch \n",
      "Loss = 0.00916533 at epoch 14 completed in 0m 17s\n",
      "Loss 0.009235482596393146 is bigger than Loss 0.009147168656012842 in the prev epoch \n",
      "Loss = 0.00923548 at epoch 15 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00906183 at epoch 16 completed in 0m 17s\n",
      "Loss 0.009070906987679858 is bigger than Loss 0.009061826845364912 in the prev epoch \n",
      "Loss = 0.00907091 at epoch 17 completed in 0m 17s\n",
      "Loss 0.009085742344281507 is bigger than Loss 0.009061826845364912 in the prev epoch \n",
      "Loss = 0.00908574 at epoch 18 completed in 0m 18s\n",
      "save the weights\n",
      "Loss = 0.00897718 at epoch 19 completed in 0m 18s\n",
      "Loss 0.009093785033162152 is bigger than Loss 0.008977181331387588 in the prev epoch \n",
      "Loss = 0.00909379 at epoch 20 completed in 0m 17s\n",
      "Loss 0.009155443111168489 is bigger than Loss 0.008977181331387588 in the prev epoch \n",
      "Loss = 0.00915544 at epoch 21 completed in 0m 17s\n",
      "Loss 0.00904173752559083 is bigger than Loss 0.008977181331387588 in the prev epoch \n",
      "Loss = 0.00904174 at epoch 22 completed in 0m 17s\n",
      "Loss 0.009022277686744925 is bigger than Loss 0.008977181331387588 in the prev epoch \n",
      "Loss = 0.00902228 at epoch 23 completed in 0m 17s\n",
      "Learning rate changed from 0.0004 to 8e-05\n",
      "Loss 0.009062985650130683 is bigger than Loss 0.008977181331387588 in the prev epoch \n",
      "Loss = 0.00906299 at epoch 24 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00896628 at epoch 25 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00894432 at epoch 26 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00889957 at epoch 27 completed in 0m 17s\n",
      "Loss 0.008925379027745553 is bigger than Loss 0.008899568972576941 in the prev epoch \n",
      "Loss = 0.00892538 at epoch 28 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00885143 at epoch 29 completed in 0m 17s\n",
      "Loss 0.008879549189337662 is bigger than Loss 0.008851432733769931 in the prev epoch \n",
      "Loss = 0.00887955 at epoch 30 completed in 0m 17s\n",
      "Loss 0.008919524893696823 is bigger than Loss 0.008851432733769931 in the prev epoch \n",
      "Loss = 0.00891952 at epoch 31 completed in 0m 17s\n",
      "Loss 0.008939936757087708 is bigger than Loss 0.008851432733769931 in the prev epoch \n",
      "Loss = 0.00893994 at epoch 32 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00881977 at epoch 33 completed in 0m 18s\n",
      "Loss 0.008829954984997002 is bigger than Loss 0.008819772283147483 in the prev epoch \n",
      "Loss = 0.00882995 at epoch 34 completed in 0m 18s\n",
      "Loss 0.008939822018146509 is bigger than Loss 0.008819772283147483 in the prev epoch \n",
      "Loss = 0.00893982 at epoch 35 completed in 0m 17s\n",
      "Loss 0.00887866521786366 is bigger than Loss 0.008819772283147483 in the prev epoch \n",
      "Loss = 0.00887867 at epoch 36 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00880628 at epoch 37 completed in 0m 17s\n",
      "Loss 0.008820811858666795 is bigger than Loss 0.008806280658713408 in the prev epoch \n",
      "Loss = 0.00882081 at epoch 38 completed in 0m 17s\n",
      "Loss 0.008853186201304195 is bigger than Loss 0.008806280658713408 in the prev epoch \n",
      "Loss = 0.00885319 at epoch 39 completed in 0m 17s\n",
      "Loss 0.008842754709933485 is bigger than Loss 0.008806280658713408 in the prev epoch \n",
      "Loss = 0.00884275 at epoch 40 completed in 0m 17s\n",
      "Loss 0.00883012471188392 is bigger than Loss 0.008806280658713408 in the prev epoch \n",
      "Loss = 0.00883012 at epoch 41 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00872583 at epoch 42 completed in 0m 17s\n",
      "Loss 0.008863196508692843 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00886320 at epoch 43 completed in 0m 18s\n",
      "Loss 0.00887047950444477 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00887048 at epoch 44 completed in 0m 17s\n",
      "Loss 0.00885336274015052 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00885336 at epoch 45 completed in 0m 18s\n",
      "Loss 0.0088212134981794 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00882121 at epoch 46 completed in 0m 17s\n",
      "Learning rate changed from 8e-05 to 1.6000000000000003e-05\n",
      "Loss 0.008837322398488013 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00883732 at epoch 47 completed in 0m 18s\n",
      "Loss 0.008738059530566845 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00873806 at epoch 48 completed in 0m 18s\n",
      "Loss 0.00883127905960594 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00883128 at epoch 49 completed in 0m 18s\n",
      "Loss 0.008904758202178138 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00890476 at epoch 50 completed in 0m 17s\n",
      "Loss 0.008778912913320319 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00877891 at epoch 51 completed in 0m 17s\n",
      "Learning rate changed from 1.6000000000000003e-05 to 3.2000000000000007e-06\n",
      "Loss 0.008739091489197949 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00873909 at epoch 52 completed in 0m 17s\n",
      "Loss 0.0087557121313044 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00875571 at epoch 53 completed in 0m 17s\n",
      "Loss 0.008761877826015865 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00876188 at epoch 54 completed in 0m 17s\n",
      "Loss 0.008737391845456192 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00873739 at epoch 55 completed in 0m 17s\n",
      "Loss 0.008794080492641246 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00879408 at epoch 56 completed in 0m 17s\n",
      "Learning rate changed from 3.2000000000000007e-06 to 6.400000000000001e-07\n",
      "Loss 0.008811472376276342 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00881147 at epoch 57 completed in 0m 17s\n",
      "Loss 0.008864871958004575 is bigger than Loss 0.008725829515606165 in the prev epoch \n",
      "Loss = 0.00886487 at epoch 58 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00872168 at epoch 59 completed in 0m 17s\n",
      "Loss 0.00875637888509248 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00875638 at epoch 60 completed in 0m 18s\n",
      "Loss 0.008765489109126584 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00876549 at epoch 61 completed in 0m 18s\n",
      "Loss 0.008791705287460769 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00879171 at epoch 62 completed in 0m 17s\n",
      "Loss 0.008754590599398525 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00875459 at epoch 63 completed in 0m 17s\n",
      "Learning rate changed from 6.400000000000001e-07 to 1.2800000000000003e-07\n",
      "Loss 0.00873714099372072 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00873714 at epoch 64 completed in 0m 18s\n",
      "Loss 0.008788262394123844 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00878826 at epoch 65 completed in 0m 17s\n",
      "Loss 0.008782021315502266 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00878202 at epoch 66 completed in 0m 17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.008826405927538871 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00882641 at epoch 67 completed in 0m 17s\n",
      "Loss 0.008829200214573313 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00882920 at epoch 68 completed in 0m 17s\n",
      "Learning rate changed from 1.2800000000000003e-07 to 2.5600000000000008e-08\n",
      "Loss 0.008822385767208679 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00882239 at epoch 69 completed in 0m 17s\n",
      "Loss 0.008813083078712225 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00881308 at epoch 70 completed in 0m 17s\n",
      "Loss 0.008790944955710853 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00879094 at epoch 71 completed in 0m 17s\n",
      "Loss 0.008764562476426364 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00876456 at epoch 72 completed in 0m 17s\n",
      "Loss 0.00884087363124958 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00884087 at epoch 73 completed in 0m 17s\n",
      "Learning rate changed from 2.5600000000000008e-08 to 5.120000000000001e-09\n",
      "Loss 0.008819387560444224 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00881939 at epoch 74 completed in 0m 18s\n",
      "Loss 0.008831625657954386 is bigger than Loss 0.008721681285117352 in the prev epoch \n",
      "Loss = 0.00883163 at epoch 75 completed in 0m 17s\n",
      "save the weights\n",
      "Loss = 0.00869971 at epoch 76 completed in 0m 18s\n",
      "save the weights\n",
      "Loss = 0.00868373 at epoch 77 completed in 0m 17s\n",
      "Loss 0.008784293396664518 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00878429 at epoch 78 completed in 0m 18s\n",
      "Loss 0.008810448806200702 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00881045 at epoch 79 completed in 0m 17s\n",
      "Loss 0.008818366651290226 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00881837 at epoch 80 completed in 0m 18s\n",
      "Loss 0.008703738597354722 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00870374 at epoch 81 completed in 0m 18s\n",
      "Learning rate changed from 5.120000000000001e-09 to 1.0240000000000002e-09\n",
      "Loss 0.008841421362012627 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00884142 at epoch 82 completed in 0m 18s\n",
      "Loss 0.008769134678212656 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00876913 at epoch 83 completed in 0m 18s\n",
      "Loss 0.008883102983236314 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00888310 at epoch 84 completed in 0m 18s\n",
      "Loss 0.008726326056889127 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00872633 at epoch 85 completed in 0m 17s\n",
      "Loss 0.00876933811232448 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00876934 at epoch 86 completed in 0m 17s\n",
      "Learning rate changed from 1.0240000000000002e-09 to 2.0480000000000003e-10\n",
      "Loss 0.008736883256850495 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00873688 at epoch 87 completed in 0m 18s\n",
      "Loss 0.008707524576623524 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00870752 at epoch 88 completed in 0m 17s\n",
      "Loss 0.008828394427629454 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00882839 at epoch 89 completed in 0m 17s\n",
      "Loss 0.008775037739958086 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00877504 at epoch 90 completed in 0m 17s\n",
      "Loss 0.008779110772801297 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00877911 at epoch 91 completed in 0m 17s\n",
      "Learning rate changed from 2.0480000000000003e-10 to 4.0960000000000006e-11\n",
      "Loss 0.008864457013883758 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00886446 at epoch 92 completed in 0m 17s\n",
      "Loss 0.008734065467225651 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00873407 at epoch 93 completed in 0m 17s\n",
      "Loss 0.008796844391950538 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00879684 at epoch 94 completed in 0m 18s\n",
      "Loss 0.00877151573742075 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00877152 at epoch 95 completed in 0m 18s\n",
      "Loss 0.008747850358486174 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00874785 at epoch 96 completed in 0m 18s\n",
      "Learning rate changed from 4.0960000000000006e-11 to 8.192e-12\n",
      "Loss 0.008735990171719879 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00873599 at epoch 97 completed in 0m 18s\n",
      "Loss 0.0087700357328036 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00877004 at epoch 98 completed in 0m 18s\n",
      "Loss 0.008739729285506266 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00873973 at epoch 99 completed in 0m 18s\n",
      "Loss 0.008800459892622061 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00880046 at epoch 100 completed in 0m 18s\n",
      "Loss 0.008814029049660475 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00881403 at epoch 101 completed in 0m 17s\n",
      "Learning rate changed from 8.192e-12 to 1.6384000000000002e-12\n",
      "Loss 0.008829586826530948 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00882959 at epoch 102 completed in 0m 17s\n",
      "Loss 0.008796698866145949 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00879670 at epoch 103 completed in 0m 18s\n",
      "Loss 0.008765762911311214 is bigger than Loss 0.00868373353566442 in the prev epoch \n",
      "Loss = 0.00876576 at epoch 104 completed in 0m 18s\n",
      "save the weights\n",
      "Loss = 0.00868325 at epoch 105 completed in 0m 17s\n",
      "Loss 0.008818115014582876 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00881812 at epoch 106 completed in 0m 17s\n",
      "Loss 0.008852929661848712 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00885293 at epoch 107 completed in 0m 18s\n",
      "Loss 0.008736093615048696 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00873609 at epoch 108 completed in 0m 18s\n",
      "Loss 0.008779697326411094 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00877970 at epoch 109 completed in 0m 17s\n",
      "Learning rate changed from 1.6384000000000002e-12 to 3.2768e-13\n",
      "Loss 0.00871117181543793 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00871117 at epoch 110 completed in 0m 17s\n",
      "Loss 0.008805472563420024 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00880547 at epoch 111 completed in 0m 18s\n",
      "Loss 0.008901120030454228 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00890112 at epoch 112 completed in 0m 18s\n",
      "Loss 0.00884421713251088 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00884422 at epoch 113 completed in 0m 18s\n",
      "Loss 0.00876178075559437 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00876178 at epoch 114 completed in 0m 17s\n",
      "Learning rate changed from 3.2768e-13 to 6.5536e-14\n",
      "Loss 0.008835645206272603 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00883565 at epoch 115 completed in 0m 18s\n",
      "Loss 0.008818109133946046 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00881811 at epoch 116 completed in 0m 18s\n",
      "Loss 0.008803292443709714 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00880329 at epoch 117 completed in 0m 18s\n",
      "Loss 0.00881334568506905 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00881335 at epoch 118 completed in 0m 17s\n",
      "Loss 0.008801964257976844 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00880196 at epoch 119 completed in 0m 17s\n",
      "Learning rate changed from 6.5536e-14 to 1.3107200000000001e-14\n",
      "Loss 0.00886444823284234 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00886445 at epoch 120 completed in 0m 17s\n",
      "Loss 0.008836652032498801 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00883665 at epoch 121 completed in 0m 18s\n",
      "Loss 0.008766355777957612 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00876636 at epoch 122 completed in 0m 17s\n",
      "Loss 0.008752049485753692 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00875205 at epoch 123 completed in 0m 17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.008746227941342763 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00874623 at epoch 124 completed in 0m 17s\n",
      "Learning rate changed from 1.3107200000000001e-14 to 2.6214400000000003e-15\n",
      "Loss 0.008822335455832737 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00882234 at epoch 125 completed in 0m 18s\n",
      "Loss 0.008803828260196106 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00880383 at epoch 126 completed in 0m 18s\n",
      "Loss 0.008694335485675508 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00869434 at epoch 127 completed in 0m 18s\n",
      "Loss 0.008819876691060408 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00881988 at epoch 128 completed in 0m 18s\n",
      "Loss 0.008757693353774295 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00875769 at epoch 129 completed in 0m 18s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 2.6214400000000003e-15 to 1e-15\n",
      "Loss 0.00875601801755173 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00875602 at epoch 130 completed in 0m 18s\n",
      "Loss 0.008751187666452354 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00875119 at epoch 131 completed in 0m 18s\n",
      "Loss 0.008749024969126497 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00874902 at epoch 132 completed in 0m 19s\n",
      "Loss 0.008748727524653081 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00874873 at epoch 133 completed in 0m 18s\n",
      "Loss 0.008688926869737249 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00868893 at epoch 134 completed in 0m 18s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008835471501307826 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00883547 at epoch 135 completed in 0m 18s\n",
      "Loss 0.008818105914230859 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00881811 at epoch 136 completed in 0m 18s\n",
      "Loss 0.008761302122314062 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00876130 at epoch 137 completed in 0m 18s\n",
      "Loss 0.008799167329977663 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00879917 at epoch 138 completed in 0m 18s\n",
      "Loss 0.008828370998214399 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00882837 at epoch 139 completed in 0m 18s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008775803825951048 is bigger than Loss 0.008683245496026106 in the prev epoch \n",
      "Loss = 0.00877580 at epoch 140 completed in 0m 18s\n",
      "save the weights\n",
      "Loss = 0.00867625 at epoch 141 completed in 0m 17s\n",
      "Loss 0.008794569436992918 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00879457 at epoch 142 completed in 0m 18s\n",
      "Loss 0.008797188395900386 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00879719 at epoch 143 completed in 0m 18s\n",
      "Loss 0.008806904372093928 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00880690 at epoch 144 completed in 0m 18s\n",
      "Loss 0.008779757549720154 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00877976 at epoch 145 completed in 0m 18s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.00887107491227133 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00887107 at epoch 146 completed in 0m 18s\n",
      "Loss 0.008786344594721282 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00878634 at epoch 147 completed in 0m 17s\n",
      "Loss 0.008794635747160233 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00879464 at epoch 148 completed in 0m 18s\n",
      "Loss 0.008743320092824953 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00874332 at epoch 149 completed in 0m 18s\n",
      "Loss 0.00884893379573311 is bigger than Loss 0.008676252999742113 in the prev epoch \n",
      "Loss = 0.00884893 at epoch 150 completed in 0m 18s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0004, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = CosineLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss = 0.00937084#float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 150, 21000, 0, 300, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,1)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind,1,0]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/CustomLoss_new/ALEX_NET_CustomLoss_new_21000_DISTANCE_50+100+150_epochs.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/CustomLoss_new/ALEX_NET_CustomLoss_new_21000_DISTANCE_50+100+150_epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([300, 1])\n",
      "num of batches ---> 15\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 20\n",
    "n_examples = 300\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred[start:end] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "MSElossRad==0.000752356489072857 ABSlossRad==0.022497525215148927 RELlossRad0.014420737443021305\n",
      "MSElossDeg==0.043106851513155145 ABSlossDeg==1.289013244317183 RELlossDeg0.8262473929513993\n"
     ]
    }
   ],
   "source": [
    "#---------------ANGLE------------------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "MSElossNor==0.016458721477538275 ABSlossNor==0.08270256261030833 RELlossNor0.1648054625315126\n",
      "MSEloss==5.266790872812248 ABSloss==26.464820035298665 RELloss52.73774801008403\n"
     ]
    }
   ],
   "source": [
    "#---------------DISTANCE------------------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,1,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,1,0])/ytestT[:,1,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossNor = torch.mean(torch.pow(dif,2))\n",
    "ABSlossNor = torch.mean(dif)\n",
    "RELlossNor = torch.mean(dif1)\n",
    "MSEloss = MSElossNor*320\n",
    "ABSloss = ABSlossNor*320\n",
    "RELloss = RELlossNor*320\n",
    "print(\"MSElossNor==\"+str(MSElossNor),\"ABSlossNor==\"+str(ABSlossNor),\"RELlossNor\"+str(RELlossNor))\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss),\"RELloss\"+str(RELloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 1])\n",
      "torch.Size([300, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(finalpred.size())\n",
    "print(ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cc4486cd909f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_figheight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_figwidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m143\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff876551e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0,0]*(180/np.pi)))\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Pred angle===\"+str(finalpred.data[ind,0]*(180/np.pi)))\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "#print(finalpred.data[n,0]*(180/np.pi))\n",
    "#print(ytestT[ind,0,0]*(180/np.pi))\n",
    "print(ytestT[ind,0,0]*(180/np.pi)-finalpred.data[ind,0]*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0026445388793945312\n"
     ]
    }
   ],
   "source": [
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "tic=time.time()\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "intr=time.time()-tic\n",
    "print(intr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ABSlossRad*(180/np.pi))\n",
    "print(MSEloss*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(ytestT[:,0]*(180/np.pi))\n",
    "#print(ytestT[:,0,2]*(180/np.pi))\n",
    "a = ytestT[:,0,0]*(180/np.pi)\n",
    "print(a.size())\n",
    "np.savetxt(\"test.csv\", a.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "#np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalpred.data[:,0]*(180/np.pi))\n",
    "print(finalpred.data[:,0]*(180/np.pi))\n",
    "b = finalpred.data[:,0]*(180/np.pi)\n",
    "c=torch.abs(ytestT[:,0,0]*(180/np.pi)- finalpred.data[:,0]*(180/np.pi))\n",
    "np.savetxt(\"pred.csv\", b.numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"diff.csv\", c.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred.data[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "test = cv2.imread(\"./Test_Net_image/4.JPG\")\n",
    "print(test.shape)\n",
    "test = imresize(test,(180,320,3))\n",
    "#imshow(test)\n",
    "test = test.transpose(2,0,1)\n",
    "test = np.reshape(test,(1,3,180,320))\n",
    "test = test.astype(np.float32)\n",
    "testPT = torch.from_numpy(test).float()\n",
    "testPT = batch_rgb_to_bgr(testPT)\n",
    "testPT = torch.div(testPT,255.0)\n",
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "testPT = norm(testPT)\n",
    "'''\n",
    "ind = 2000\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0]*(180/np.pi)), ytestT[ind,0])\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "#'''\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Angle===\"+str(test_pred.data[0,0]*(180/np.pi)), test_pred.data[0,0])\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
