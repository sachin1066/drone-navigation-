{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvrlab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net_152 = models.resnet152(pretrained=True)\n",
    "#dense161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net_152\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.bias.data.zero_()    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Sequential(nn.BatchNorm2d(2048),nn.ReLU(),nn.Conv2d(2048,1024,1))\n",
    "conv1 = InitializeWeights(conv1)\n",
    "conv2 = nn.Sequential(nn.BatchNorm2d(1024),nn.ReLU(),nn.Conv2d(1024,128,5))\n",
    "conv2 = InitializeWeights(conv2)\n",
    "conv3 = nn.Sequential(nn.BatchNorm2d(128),nn.ReLU(),nn.Conv2d(128,8,1))\n",
    "conv3 = InitializeWeights(conv3)\n",
    "norm1 = nn.BatchNorm2d(8)\n",
    "norm1 = InitializeWeights(norm1)\n",
    "fc1 = nn.Linear(96, 1)\n",
    "fc1 = InitializeWeights(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel4(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyModel4, self).__init__()\n",
    "        self.pretrained_model = nn.Sequential(*list(res_net_152.children())[:-2])\n",
    "        self.conv1 = conv1\n",
    "        self.conv2 = conv2\n",
    "        self.conv3 = conv3\n",
    "        self.norm1 = norm1\n",
    "        self.fc1 = fc1\n",
    "   \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm1(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #print(x.size())\n",
    "        #x = self.conv4(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModel4(res_net_152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3131344318389893\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "input=Variable(torch.randn(1,3,180,320))\n",
    "tic=time.time()\n",
    "output=net(input)\n",
    "tac=time.time()\n",
    "print(tac-tic)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n",
      "torch.Size([64, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  481\n",
      "Total number of parameters --->  60192808\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in res_net_152.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(5, 3, 180, 320))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.53532934188843\n"
     ]
    }
   ],
   "source": [
    "tic_1=time.time()\n",
    "file = h5py.File('./DATASET/CODE/NewTrainData_21000_distance.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)\n",
    "toc_1=time.time()\n",
    "print(toc_1-tic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData_random_distance_1.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#print(xtrainT.size(), xtestT.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "#xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#xtestT = torch.div(xtestT,255.0)\n",
    "#mn = [0.406,0.456,0.485]\n",
    "#sd = [0.225,0.224,0.229]\n",
    "#norm = Normalize(mn,sd)\n",
    "#xtestT = norm(xtestT)\n",
    "#print(xtestT.size(), ytestT.size())\n",
    "#print(torch.min(xtestT), torch.max(xtestT),torch.min(ytestT), torch.max(ytestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = batch_rgb_to_bgr(x)\n",
    "    x = torch.div(x,255.0)\n",
    "    mn = [0.406,0.456,0.485]\n",
    "    sd = [0.225,0.224,0.229]\n",
    "    x[:,0,:,:] = (x[:,0,:,:]-mn[0])/sd[0]\n",
    "    x[:,1,:,:] = (x[:,1,:,:]-mn[1])/sd[1]\n",
    "    x[:,2,:,:] = (x[:,2,:,:]-mn[2])/sd[2]\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff = torch.abs(tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "        loss = torch.mean(diff)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BerhuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BerhuLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        mt = tar[:,0]\n",
    "        mp = inp[:,0]\n",
    "        diff = torch.abs(mt-mp)        \n",
    "        lossval = 0.0        \n",
    "        c = 0.2 * torch.max(diff)\n",
    "        l1 = torch.mean(diff)\n",
    "        l2 = torch.mean(torch.pow(diff,2))\n",
    "        if l1 <= c:\n",
    "            lossval = l1\n",
    "        else:\n",
    "            lossval = (l2+c**2)/(2*c)\n",
    "        \n",
    "        return lossval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = torch.FloatTensor(ytrainT[5,0])\n",
    "alpha = ytrainT[5,0]\n",
    "#print(alpha.shape)\n",
    "xt = torch.FloatTensor([np.cos(alpha),np.sin(alpha)])\n",
    "print(ytrainT[5,0],xt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ytrainT[5:10,0]\n",
    "print(torch.cos(alpha[0:1]-alpha[1:2]))\n",
    "xt = torch.stack([torch.cos(alpha[0:1]),torch.sin(alpha[0:1])])\n",
    "xp = torch.stack([torch.cos(alpha[1:2]),torch.sin(alpha[1:2])])\n",
    "print(xt[0],xt[1])\n",
    "#print(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar,batch_sz):\n",
    "        alpha_t = tar[:,0]\n",
    "        alpha_p = inp[:,0]\n",
    "        #xt = torch.stack([torch.cos(alpha_t),torch.sin(alpha_t)])\n",
    "        #xp = torch.stack([torch.cos(alpha_p),torch.sin(alpha_p)])\n",
    "        #cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        #loss = cos(xt, xp)\n",
    "        #return loss\n",
    "        loss = Variable(torch.FloatTensor(batch_sz).zero_(), requires_grad=False).cuda()\n",
    "        for i in range(batch_sz):          \n",
    "            loss[i] = torch.cos(alpha_t[i:i+1]-alpha_p[i:i+1])\n",
    "            \n",
    "        lossval = 1.0-torch.mean(loss)    \n",
    "        #print(lossval)\n",
    "        return lossval\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.03778188 at epoch 1 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.03314808 at epoch 2 completed in 6m 6s\n",
      "save the weights\n",
      "Loss = 0.02665588 at epoch 3 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.02628472 at epoch 4 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.02213624 at epoch 5 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.01927624 at epoch 6 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.01727900 at epoch 7 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.01566710 at epoch 8 completed in 6m 5s\n",
      "Loss 0.01598505408989277 is bigger than Loss 0.015667104423661135 in the prev epoch \n",
      "Loss = 0.01598505 at epoch 9 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.01264641 at epoch 10 completed in 6m 4s\n",
      "save the weights\n",
      "Loss = 0.01233457 at epoch 11 completed in 6m 4s\n",
      "save the weights\n",
      "Loss = 0.01165688 at epoch 12 completed in 6m 4s\n",
      "save the weights\n",
      "Loss = 0.01034504 at epoch 13 completed in 6m 4s\n",
      "save the weights\n",
      "Loss = 0.00982790 at epoch 14 completed in 6m 4s\n",
      "save the weights\n",
      "Loss = 0.00822641 at epoch 15 completed in 6m 4s\n",
      "save the weights\n",
      "Loss = 0.00783037 at epoch 16 completed in 6m 4s\n",
      "Loss 0.008077810616920693 is bigger than Loss 0.007830373704896176 in the prev epoch \n",
      "Loss = 0.00807781 at epoch 17 completed in 6m 4s\n",
      "Loss 0.008171099382701599 is bigger than Loss 0.007830373704896176 in the prev epoch \n",
      "Loss = 0.00817110 at epoch 18 completed in 6m 5s\n",
      "Loss 0.007900291693224086 is bigger than Loss 0.007830373704896176 in the prev epoch \n",
      "Loss = 0.00790029 at epoch 19 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00740576 at epoch 20 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00657454 at epoch 21 completed in 6m 5s\n",
      "Loss 0.006989209674395408 is bigger than Loss 0.006574543629723176 in the prev epoch \n",
      "Loss = 0.00698921 at epoch 22 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00631999 at epoch 23 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00625139 at epoch 24 completed in 6m 5s\n",
      "Loss 0.006404024549560772 is bigger than Loss 0.0062513868983413235 in the prev epoch \n",
      "Loss = 0.00640402 at epoch 25 completed in 6m 5s\n",
      "Loss 0.006308674592299259 is bigger than Loss 0.0062513868983413235 in the prev epoch \n",
      "Loss = 0.00630867 at epoch 26 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00540124 at epoch 27 completed in 6m 5s\n",
      "Loss 0.005502070131881528 is bigger than Loss 0.005401239876852095 in the prev epoch \n",
      "Loss = 0.00550207 at epoch 28 completed in 6m 5s\n",
      "Loss 0.005875385166131219 is bigger than Loss 0.005401239876852095 in the prev epoch \n",
      "Loss = 0.00587539 at epoch 29 completed in 6m 5s\n",
      "Loss 0.006390882197638637 is bigger than Loss 0.005401239876852095 in the prev epoch \n",
      "Loss = 0.00639088 at epoch 30 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00523758 at epoch 31 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00491021 at epoch 32 completed in 6m 5s\n",
      "Loss 0.005096720477036027 is bigger than Loss 0.004910211287125492 in the prev epoch \n",
      "Loss = 0.00509672 at epoch 33 completed in 6m 5s\n",
      "Loss 0.005273934573801993 is bigger than Loss 0.004910211287125492 in the prev epoch \n",
      "Loss = 0.00527393 at epoch 34 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00480764 at epoch 35 completed in 6m 5s\n",
      "Loss 0.005061309539739322 is bigger than Loss 0.0048076443012830105 in the prev epoch \n",
      "Loss = 0.00506131 at epoch 36 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00477800 at epoch 37 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00440472 at epoch 38 completed in 6m 5s\n",
      "Loss 0.00452717225548354 is bigger than Loss 0.004404715738997289 in the prev epoch \n",
      "Loss = 0.00452717 at epoch 39 completed in 6m 5s\n",
      "Loss 0.004459858600177611 is bigger than Loss 0.004404715738997289 in the prev epoch \n",
      "Loss = 0.00445986 at epoch 40 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00440184 at epoch 41 completed in 6m 5s\n",
      "Loss 0.004448080366388043 is bigger than Loss 0.0044018359173531455 in the prev epoch \n",
      "Loss = 0.00444808 at epoch 42 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00410983 at epoch 43 completed in 6m 5s\n",
      "Loss 0.004367474879324141 is bigger than Loss 0.004109834586804639 in the prev epoch \n",
      "Loss = 0.00436747 at epoch 44 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00402956 at epoch 45 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00401480 at epoch 46 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00395368 at epoch 47 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00385563 at epoch 48 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00364981 at epoch 49 completed in 6m 5s\n",
      "Loss 0.003754306482641706 is bigger than Loss 0.003649812260915016 in the prev epoch \n",
      "Loss = 0.00375431 at epoch 50 completed in 6m 5s\n",
      "Loss 0.003828550382874578 is bigger than Loss 0.003649812260915016 in the prev epoch \n",
      "Loss = 0.00382855 at epoch 51 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00347812 at epoch 52 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00333031 at epoch 53 completed in 6m 5s\n",
      "Loss 0.003431651730649759 is bigger than Loss 0.0033303146690215166 in the prev epoch \n",
      "Loss = 0.00343165 at epoch 54 completed in 6m 5s\n",
      "Loss 0.0034377178524653052 is bigger than Loss 0.0033303146690215166 in the prev epoch \n",
      "Loss = 0.00343772 at epoch 55 completed in 6m 5s\n",
      "Loss 0.0033845753945650403 is bigger than Loss 0.0033303146690215166 in the prev epoch \n",
      "Loss = 0.00338458 at epoch 56 completed in 6m 5s\n",
      "Loss 0.003382019336926956 is bigger than Loss 0.0033303146690215166 in the prev epoch \n",
      "Loss = 0.00338202 at epoch 57 completed in 6m 5s\n",
      "Learning rate changed from 0.002 to 0.0004\n",
      "Loss 0.0033444106240563525 is bigger than Loss 0.0033303146690215166 in the prev epoch \n",
      "Loss = 0.00334441 at epoch 58 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00190821 at epoch 59 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00185643 at epoch 60 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00180518 at epoch 61 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00178592 at epoch 62 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00175138 at epoch 63 completed in 6m 5s\n",
      "Loss 0.0017760532893315133 is bigger than Loss 0.0017513826363331948 in the prev epoch \n",
      "Loss = 0.00177605 at epoch 64 completed in 6m 5s\n",
      "Loss 0.0017681175974855177 is bigger than Loss 0.0017513826363331948 in the prev epoch \n",
      "Loss = 0.00176812 at epoch 65 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00170165 at epoch 66 completed in 6m 5s\n",
      "Loss 0.001723470471732746 is bigger than Loss 0.0017016484952530287 in the prev epoch \n",
      "Loss = 0.00172347 at epoch 67 completed in 6m 5s\n",
      "Loss 0.0017040146871890177 is bigger than Loss 0.0017016484952530287 in the prev epoch \n",
      "Loss = 0.00170401 at epoch 68 completed in 6m 5s\n",
      "Loss 0.0017517722803373006 is bigger than Loss 0.0017016484952530287 in the prev epoch \n",
      "Loss = 0.00175177 at epoch 69 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00169403 at epoch 70 completed in 6m 5s\n",
      "Loss 0.0017051790752514034 is bigger than Loss 0.0016940347390884904 in the prev epoch \n",
      "Loss = 0.00170518 at epoch 71 completed in 6m 5s\n",
      "Loss 0.001760330718832893 is bigger than Loss 0.0016940347390884904 in the prev epoch \n",
      "Loss = 0.00176033 at epoch 72 completed in 6m 5s\n",
      "Loss 0.0017485382871331836 is bigger than Loss 0.0016940347390884904 in the prev epoch \n",
      "Loss = 0.00174854 at epoch 73 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00164038 at epoch 74 completed in 6m 5s\n",
      "Loss 0.0016910713837766124 is bigger than Loss 0.0016403788191474299 in the prev epoch \n",
      "Loss = 0.00169107 at epoch 75 completed in 6m 5s\n",
      "Loss 0.0017307892405324017 is bigger than Loss 0.0016403788191474299 in the prev epoch \n",
      "Loss = 0.00173079 at epoch 76 completed in 6m 5s\n",
      "Loss 0.0016895635614542096 is bigger than Loss 0.0016403788191474299 in the prev epoch \n",
      "Loss = 0.00168956 at epoch 77 completed in 6m 5s\n",
      "Loss 0.0016816680074512739 is bigger than Loss 0.0016403788191474299 in the prev epoch \n",
      "Loss = 0.00168167 at epoch 78 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00162021 at epoch 79 completed in 6m 5s\n",
      "Loss 0.0016315989151454975 is bigger than Loss 0.0016202063223020526 in the prev epoch \n",
      "Loss = 0.00163160 at epoch 80 completed in 6m 5s\n",
      "Loss 0.0016621446889333146 is bigger than Loss 0.0016202063223020526 in the prev epoch \n",
      "Loss = 0.00166214 at epoch 81 completed in 6m 5s\n",
      "Loss 0.0016715535209455617 is bigger than Loss 0.0016202063223020526 in the prev epoch \n",
      "Loss = 0.00167155 at epoch 82 completed in 6m 5s\n",
      "Loss 0.001655642510557974 is bigger than Loss 0.0016202063223020526 in the prev epoch \n",
      "Loss = 0.00165564 at epoch 83 completed in 6m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate changed from 0.0004 to 8e-05\n",
      "Loss 0.0016458622830094025 is bigger than Loss 0.0016202063223020526 in the prev epoch \n",
      "Loss = 0.00164586 at epoch 84 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00135425 at epoch 85 completed in 6m 5s\n",
      "Loss 0.0013705213714732191 is bigger than Loss 0.001354248203452875 in the prev epoch \n",
      "Loss = 0.00137052 at epoch 86 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00133674 at epoch 87 completed in 6m 5s\n",
      "Loss 0.0013467180594815083 is bigger than Loss 0.001336741652804626 in the prev epoch \n",
      "Loss = 0.00134672 at epoch 88 completed in 6m 5s\n",
      "Loss 0.0013578522857241972 is bigger than Loss 0.001336741652804626 in the prev epoch \n",
      "Loss = 0.00135785 at epoch 89 completed in 6m 5s\n",
      "Loss 0.0013446924654633662 is bigger than Loss 0.001336741652804626 in the prev epoch \n",
      "Loss = 0.00134469 at epoch 90 completed in 6m 5s\n",
      "Loss 0.0013490167865419817 is bigger than Loss 0.001336741652804626 in the prev epoch \n",
      "Loss = 0.00134902 at epoch 91 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00133551 at epoch 92 completed in 6m 5s\n",
      "Loss 0.0013640457651696494 is bigger than Loss 0.001335512398710656 in the prev epoch \n",
      "Loss = 0.00136405 at epoch 93 completed in 6m 5s\n",
      "Loss 0.0013498230365949898 is bigger than Loss 0.001335512398710656 in the prev epoch \n",
      "Loss = 0.00134982 at epoch 94 completed in 6m 5s\n",
      "Loss 0.00134104396415722 is bigger than Loss 0.001335512398710656 in the prev epoch \n",
      "Loss = 0.00134104 at epoch 95 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00133191 at epoch 96 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00131545 at epoch 97 completed in 6m 5s\n",
      "Loss 0.0013666103944349334 is bigger than Loss 0.0013154485445424225 in the prev epoch \n",
      "Loss = 0.00136661 at epoch 98 completed in 6m 5s\n",
      "Loss 0.00134430797521681 is bigger than Loss 0.0013154485445424225 in the prev epoch \n",
      "Loss = 0.00134431 at epoch 99 completed in 6m 5s\n",
      "Loss 0.0013180857548254407 is bigger than Loss 0.0013154485445424225 in the prev epoch \n",
      "Loss = 0.00131809 at epoch 100 completed in 6m 5s\n",
      "Loss 0.0013168617952490564 is bigger than Loss 0.0013154485445424225 in the prev epoch \n",
      "Loss = 0.00131686 at epoch 101 completed in 6m 5s\n",
      "Learning rate changed from 8e-05 to 1.6000000000000003e-05\n",
      "Loss 0.0013295282208347563 is bigger than Loss 0.0013154485445424225 in the prev epoch \n",
      "Loss = 0.00132953 at epoch 102 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00126355 at epoch 103 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00125406 at epoch 104 completed in 6m 5s\n",
      "Loss 0.0012799432329502565 is bigger than Loss 0.0012540617591991403 in the prev epoch \n",
      "Loss = 0.00127994 at epoch 105 completed in 6m 5s\n",
      "Loss 0.0012711103895383321 is bigger than Loss 0.0012540617591991403 in the prev epoch \n",
      "Loss = 0.00127111 at epoch 106 completed in 6m 5s\n",
      "Loss 0.001268672276637517 is bigger than Loss 0.0012540617591991403 in the prev epoch \n",
      "Loss = 0.00126867 at epoch 107 completed in 6m 5s\n",
      "Loss 0.001274219948034032 is bigger than Loss 0.0012540617591991403 in the prev epoch \n",
      "Loss = 0.00127422 at epoch 108 completed in 6m 5s\n",
      "Learning rate changed from 1.6000000000000003e-05 to 3.2000000000000007e-06\n",
      "Loss 0.0012763590473570399 is bigger than Loss 0.0012540617591991403 in the prev epoch \n",
      "Loss = 0.00127636 at epoch 109 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00123205 at epoch 110 completed in 6m 5s\n",
      "Loss 0.0012378242840066426 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00123782 at epoch 111 completed in 6m 5s\n",
      "Loss 0.0012405421994860816 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00124054 at epoch 112 completed in 6m 5s\n",
      "Loss 0.0012806320829103182 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00128063 at epoch 113 completed in 6m 5s\n",
      "Loss 0.0012419539643414965 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00124195 at epoch 114 completed in 6m 5s\n",
      "Learning rate changed from 3.2000000000000007e-06 to 6.400000000000001e-07\n",
      "Loss 0.0012571585975820196 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00125716 at epoch 115 completed in 6m 5s\n",
      "Loss 0.0012416282499211977 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00124163 at epoch 116 completed in 6m 5s\n",
      "Loss 0.0012485627668877507 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00124856 at epoch 117 completed in 6m 5s\n",
      "Loss 0.0012635374564381684 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00126354 at epoch 118 completed in 6m 5s\n",
      "Loss 0.0012539799003744708 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00125398 at epoch 119 completed in 6m 5s\n",
      "Learning rate changed from 6.400000000000001e-07 to 1.2800000000000003e-07\n",
      "Loss 0.0012640909405473534 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00126409 at epoch 120 completed in 6m 5s\n",
      "Loss 0.0012664296920557722 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00126643 at epoch 121 completed in 6m 5s\n",
      "Loss 0.0012612572168995808 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00126126 at epoch 122 completed in 6m 5s\n",
      "Loss 0.0012387939019138934 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00123879 at epoch 123 completed in 6m 5s\n",
      "Loss 0.0012349816115716557 is bigger than Loss 0.0012320478349036681 in the prev epoch \n",
      "Loss = 0.00123498 at epoch 124 completed in 6m 5s\n",
      "save the weights\n",
      "Loss = 0.00122431 at epoch 125 completed in 6m 5s\n",
      "Loss 0.0012492255797190892 is bigger than Loss 0.0012243119968649047 in the prev epoch \n",
      "Loss = 0.00124923 at epoch 126 completed in 6m 5s\n",
      "Loss 0.0012680881263347017 is bigger than Loss 0.0012243119968649047 in the prev epoch \n",
      "Loss = 0.00126809 at epoch 127 completed in 6m 5s\n",
      "Loss 0.0012642494802774693 is bigger than Loss 0.0012243119968649047 in the prev epoch \n",
      "Loss = 0.00126425 at epoch 128 completed in 6m 5s\n",
      "Loss 0.0012479520919212901 is bigger than Loss 0.0012243119968649047 in the prev epoch \n",
      "Loss = 0.00124795 at epoch 129 completed in 6m 5s\n",
      "Learning rate changed from 1.2800000000000003e-07 to 2.5600000000000008e-08\n",
      "Loss 0.0012449064575181124 is bigger than Loss 0.0012243119968649047 in the prev epoch \n",
      "Loss = 0.00124491 at epoch 130 completed in 6m 5s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=.002, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = CosineLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss =float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 130, 21000, 0, 33, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,1)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind,1,0]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/CustomLoss_new/RES_NET_CustomLoss_new_21000_DISTANCE_130_epochs.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainT = xtrainT.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5600000000000008e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/CustomLoss_new/RES_NET_CustomLoss_new_21000_DISTANCE_130_epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([300, 1])\n",
      "num of batches ---> 15\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 20\n",
    "n_examples = 300\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred[start:end] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------Angle----------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "MSElossNor==0.00020678338005242658 ABSlossNor==0.01070575326681137 RELlossNor0.03382188576588912\n",
      "MSEloss==0.0661706816167765 ABSloss==3.4258410453796384 RELloss10.823003445084519\n"
     ]
    }
   ],
   "source": [
    "#---------------DISTANCE------------------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,1,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,1,0])/ytestT[:,1,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSElossNor = torch.mean(torch.pow(dif,2))\n",
    "ABSlossNor = torch.mean(dif)\n",
    "RELlossNor = torch.mean(dif1)\n",
    "MSEloss = MSElossNor*320\n",
    "ABSloss = ABSlossNor*320\n",
    "RELloss = RELlossNor*320\n",
    "print(\"MSElossNor==\"+str(MSElossNor),\"ABSlossNor==\"+str(ABSlossNor),\"RELlossNor\"+str(RELlossNor))\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss),\"RELloss\"+str(RELloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03551363945007324\n"
     ]
    }
   ],
   "source": [
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "tic=time.time()\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "intr=time.time()-tic\n",
    "print(intr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalpred.size())\n",
    "print(ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0,0]*(180/np.pi)))\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Pred angle===\"+str(finalpred.data[ind,0]*(180/np.pi)))\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "#print(finalpred.data[n,0]*(180/np.pi))\n",
    "#print(ytestT[ind,0,0]*(180/np.pi))\n",
    "print(ytestT[ind,0,0]*(180/np.pi)-finalpred.data[ind,0]*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ABSlossRad*(180/np.pi))\n",
    "print(MSEloss*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(ytestT[:,0]*(180/np.pi))\n",
    "#print(ytestT[:,0,2]*(180/np.pi))\n",
    "a = ytestT[:,0,0]*(180/np.pi)\n",
    "print(a.size())\n",
    "np.savetxt(\"test.csv\", a.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "#np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalpred.data[:,0]*(180/np.pi))\n",
    "print(finalpred.data[:,0]*(180/np.pi))\n",
    "b = finalpred.data[:,0]*(180/np.pi)\n",
    "c=torch.abs(ytestT[:,0,0]*(180/np.pi)- finalpred.data[:,0]*(180/np.pi))\n",
    "np.savetxt(\"pred.csv\", b.numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"diff.csv\", c.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred.data[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "test = cv2.imread(\"./Test_Net_image/4.JPG\")\n",
    "print(test.shape)\n",
    "test = imresize(test,(180,320,3))\n",
    "#imshow(test)\n",
    "test = test.transpose(2,0,1)\n",
    "test = np.reshape(test,(1,3,180,320))\n",
    "test = test.astype(np.float32)\n",
    "testPT = torch.from_numpy(test).float()\n",
    "testPT = batch_rgb_to_bgr(testPT)\n",
    "testPT = torch.div(testPT,255.0)\n",
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "testPT = norm(testPT)\n",
    "'''\n",
    "ind = 2000\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0]*(180/np.pi)), ytestT[ind,0])\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "#'''\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Angle===\"+str(test_pred.data[0,0]*(180/np.pi)), test_pred.data[0,0])\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
