{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvrlab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import re\n",
    "#import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "incep_v3 = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incep_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#incep_v3.AuxLogits\n",
    "list(incep_v3.children())[14:17]\n",
    "\n",
    "#list(incep_v3.children())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.bias.data.zero_()\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_l = nn.Sequential(nn.BatchNorm2d(2048),nn.ReLU(),nn.Conv2d(2048,1024,1))\n",
    "conv1_l = InitializeWeights(conv1_l)\n",
    "conv2_l = nn.Sequential(nn.BatchNorm2d(1024),nn.ReLU(),nn.Conv2d(1024,512,(2,2),(1,2)))\n",
    "conv2_l = InitializeWeights(conv2_l)\n",
    "conv3_l = nn.Sequential(nn.BatchNorm2d(512),nn.ReLU(),nn.Conv2d(512,128,(3,3)))\n",
    "conv3_l = InitializeWeights(conv3_l)\n",
    "norm1_l = nn.BatchNorm2d(128)\n",
    "norm1_l = InitializeWeights(norm1_l)\n",
    "fc1_l = nn.Sequential(nn.Linear(256, 1))\n",
    "fc1_l = InitializeWeights(fc1_l)\n",
    "#############__________Aux_____________#############\n",
    "conv1_aux = nn.Sequential(nn.BatchNorm2d(768),nn.ReLU(),nn.Conv2d(768,128,4,(1,2),(1,0)))\n",
    "conv1_aux = InitializeWeights(conv1_aux)\n",
    "conv2_aux = nn.Sequential(nn.BatchNorm2d(128),nn.ReLU(),nn.Conv2d(128,32,(1,2)))\n",
    "conv2_aux = InitializeWeights(conv2_aux)\n",
    "norm1_aux = nn.BatchNorm2d(32)\n",
    "norm1_aux = InitializeWeights(norm1_aux)\n",
    "fc1_aux = nn.Sequential(nn.Linear(640, 1))\n",
    "fc1_aux = InitializeWeights(fc1_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IV3_git(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(IV3_git, self).__init__()\n",
    "        self.layer0_2 = nn.Sequential(*list(incep_v3.children())[0:3])#[0,1,2]\n",
    "        self.layer3_4 =  nn.Sequential(*list(incep_v3.children())[3:5])#[3,4]\n",
    "        self.layer5_12 =  nn.Sequential(*list(incep_v3.children())[5:13])#[5,12]\n",
    "        self.layer14_16 =  nn.Sequential(*list(incep_v3.children())[14:17])#[14,6]\n",
    "        ############\n",
    "        self.aauxLogits = nn.Sequential(*list(incep_v3.AuxLogits.children())[:-1])\n",
    "        #######################\n",
    "        self.conv1_l = conv1_l\n",
    "        self.conv2_l = conv2_l\n",
    "        self.conv3_l = conv3_l\n",
    "        self.norm1_l = norm1_l\n",
    "        self.fc1_l = fc1_l\n",
    "        ########################   \n",
    "        self.conv1_aux = conv1_aux\n",
    "        self.conv2_aux = conv2_aux\n",
    "        #self.conv3_aux = conv3_aux\n",
    "        self.norm1_aux = norm1_aux\n",
    "        self.fc1_aux= fc1_aux\n",
    "        \n",
    "        \n",
    "            \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        y1 = self.layer0_2(x)\n",
    "        y1 = F.max_pool2d(y1, kernel_size=3, stride=2)\n",
    "        y1 = self.layer3_4(y1)\n",
    "        y1 = F.max_pool2d(y1, kernel_size=3, stride=2)\n",
    "        y1 = self.layer5_12(y1)\n",
    "        #################\n",
    "        #if self.aux_logits:\n",
    "        aux = self.aauxLogits(y1)\n",
    "        aux = self.conv1_aux(aux)\n",
    "        aux = self.conv2_aux(aux)\n",
    "        #aux = self.conv3_aux(aux)\n",
    "        aux = self.norm1_aux(aux)\n",
    "        aux = aux.view(-1, self.num_flat_features(aux))\n",
    "        aux = self.fc1_aux(aux)\n",
    "        ##################\n",
    "        y1 = self.layer14_16(y1)\n",
    "        #y1 = F.avg_pool2d(y1, kernel_size=8)\n",
    "        y1 = F.dropout(y1)\n",
    "        y1 = self.conv1_l(y1)\n",
    "        y1 = self.conv2_l(y1)\n",
    "        y1 = self.conv3_l(y1)\n",
    "        y1 = self.norm1_l(y1)\n",
    "        y1 = y1.view(-1, self.num_flat_features(y1))\n",
    "        y1= self.fc1_l(y1)\n",
    "        ##################\n",
    "        \n",
    "        return aux,y1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = IV3_git(incep_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aux\t torch.Size([1, 1])\n",
      "incep\t torch.Size([1, 1])\n",
      "0.4677455425262451\n"
     ]
    }
   ],
   "source": [
    "input1=Variable(torch.Tensor(1,3,180,320))\n",
    "tic=time.time()\n",
    "aux,incep=net(input1)\n",
    "tac=time.time()\n",
    "print(\"aux\\t \"+str(aux.size()))\n",
    "print(\"incep\\t \"+str(incep.size()))\n",
    "print(tac-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incep)\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(incep_v3.parameters())))\n",
    "for params in incep_v3.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  316\n",
      "Total number of parameters --->  30720450\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in net.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############      rough\n",
    "# ind1 = np.random.randint(0,24)\n",
    "# print(ind1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.75822901725769\n"
     ]
    }
   ],
   "source": [
    "tic_1=time.time()\n",
    "file = h5py.File('./DATASET/CODE/NewTrainData_21000_distance.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)\n",
    "toc_1=time.time()\n",
    "print(toc_1-tic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData_random_300_distance.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#print(xtrainT.size(), xtestT.size())\n",
    "#print(xtrainT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denom = [255.0,255.0,255.0]\n",
    "# for t, m in zip(xtrainT, denom):\n",
    "#          t.div_(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "#xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))\n",
    "print(torch.min(xtrainT), torch.max(xtrainT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT[1,0,0,0])\n",
    "type(xtrainT)\n",
    "#xtrainT.index(255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = batch_rgb_to_bgr(x)\n",
    "    x = torch.div(x,255.0)\n",
    "    mn = [0.406,0.456,0.485]\n",
    "    sd = [0.225,0.224,0.229]\n",
    "    x[:,0,:,:] = (x[:,0,:,:]-mn[0])/sd[0]\n",
    "    x[:,1,:,:] = (x[:,1,:,:]-mn[1])/sd[1]\n",
    "    x[:,2,:,:] = (x[:,2,:,:]-mn[2])/sd[2]\n",
    "    #x=torch.div(torch.sub(x-mn),sd)\n",
    "    \n",
    "    fx1,fx2 = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx1,fx2,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp1,inp2, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff1 = torch.abs(tar[:,0]-inp1[:,0]) #*(180/np.pi)\n",
    "        diff2 = torch.abs(tar[:,0]-inp2[:,0])\n",
    "        diff = diff1+diff2\n",
    "        loss = torch.mean(diff)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.008140780621518686 is bigger than Loss 0.00812351 in the prev epoch \n",
      "Loss = 0.00814078 at epoch 1 completed in 2m 25s\n",
      "save the weights\n",
      "Loss = 0.00802259 at epoch 2 completed in 2m 29s\n",
      "save the weights\n",
      "Loss = 0.00800913 at epoch 3 completed in 2m 21s\n",
      "Loss 0.008099024213983546 is bigger than Loss 0.008009131491139887 in the prev epoch \n",
      "Loss = 0.00809902 at epoch 4 completed in 2m 23s\n",
      "Loss 0.008062950954107305 is bigger than Loss 0.008009131491139887 in the prev epoch \n",
      "Loss = 0.00806295 at epoch 5 completed in 2m 22s\n",
      "Loss 0.008091624966999963 is bigger than Loss 0.008009131491139887 in the prev epoch \n",
      "Loss = 0.00809162 at epoch 6 completed in 2m 22s\n",
      "save the weights\n",
      "Loss = 0.00794868 at epoch 7 completed in 2m 23s\n",
      "Loss 0.008054631477266202 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00805463 at epoch 8 completed in 2m 19s\n",
      "Loss 0.008051677241123156 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00805168 at epoch 9 completed in 2m 19s\n",
      "Loss 0.008040796675985415 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00804080 at epoch 10 completed in 2m 20s\n",
      "Loss 0.008039153920530923 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00803915 at epoch 11 completed in 2m 19s\n",
      "Learning rate changed from 5.120000000000001e-09 to 1.0240000000000002e-09\n",
      "Loss 0.00802791356996056 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00802791 at epoch 12 completed in 2m 19s\n",
      "Loss 0.008199239908052346 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00819924 at epoch 13 completed in 2m 19s\n",
      "Loss 0.008009746069798165 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00800975 at epoch 14 completed in 2m 19s\n",
      "Loss 0.00801965804615369 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00801966 at epoch 15 completed in 2m 19s\n",
      "Loss 0.008015291658895345 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00801529 at epoch 16 completed in 2m 19s\n",
      "Learning rate changed from 1.0240000000000002e-09 to 2.0480000000000003e-10\n",
      "Loss 0.008059631694950877 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00805963 at epoch 17 completed in 2m 19s\n",
      "Loss 0.008067223879819105 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00806722 at epoch 18 completed in 2m 19s\n",
      "Loss 0.008074245263733685 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00807425 at epoch 19 completed in 2m 19s\n",
      "Loss 0.008049672947353913 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00804967 at epoch 20 completed in 2m 19s\n",
      "Loss 0.00811990284405294 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00811990 at epoch 21 completed in 2m 19s\n",
      "Learning rate changed from 2.0480000000000003e-10 to 4.0960000000000006e-11\n",
      "Loss 0.007977928241182656 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00797793 at epoch 22 completed in 2m 19s\n",
      "Loss 0.008015688905669824 is bigger than Loss 0.007948679092251467 in the prev epoch \n",
      "Loss = 0.00801569 at epoch 23 completed in 2m 19s\n",
      "save the weights\n",
      "Loss = 0.00790718 at epoch 24 completed in 2m 19s\n",
      "Loss 0.008099119703374097 is bigger than Loss 0.00790717909999547 in the prev epoch \n",
      "Loss = 0.00809912 at epoch 25 completed in 2m 19s\n",
      "Loss 0.008053774526342752 is bigger than Loss 0.00790717909999547 in the prev epoch \n",
      "Loss = 0.00805377 at epoch 26 completed in 2m 19s\n",
      "Loss 0.008105632015282194 is bigger than Loss 0.00790717909999547 in the prev epoch \n",
      "Loss = 0.00810563 at epoch 27 completed in 2m 19s\n",
      "Loss 0.008031945860767284 is bigger than Loss 0.00790717909999547 in the prev epoch \n",
      "Loss = 0.00803195 at epoch 28 completed in 2m 20s\n",
      "Learning rate changed from 4.0960000000000006e-11 to 8.192e-12\n",
      "Loss 0.008032849538583492 is bigger than Loss 0.00790717909999547 in the prev epoch \n",
      "Loss = 0.00803285 at epoch 29 completed in 2m 21s\n",
      "Loss 0.007976118239041948 is bigger than Loss 0.00790717909999547 in the prev epoch \n",
      "Loss = 0.00797612 at epoch 30 completed in 2m 19s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=5.120000000000001e-09, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = CosineLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss =float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 30, 21000, 0, 50, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,1)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind,1,0]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/CustomLoss_new/CustomLoss_new_21000_INCEP_NET_new_DISTANCE_130__epochs.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.192e-12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/CustomLoss_new/CustomLoss_new_21000_INCEP_NET_new_DISTANCE_130__epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([300, 1])\n",
      "finalpred size is --->  torch.Size([300, 1])\n",
      "num of batches ---> 15\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 20\n",
    "n_examples = 300\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred1 = Variable(torch.zeros((n_examples,1)))\n",
    "finalpred2 = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred1.size())\n",
    "print(\"finalpred size is ---> \", finalpred2.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output1,output2 = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred1[start:end] = output1\n",
    "    finalpred2[start:end] = output2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = finalpred1.data.numpy()\n",
    "data2 = finalpred2.data.numpy()\n",
    "print(data1.shape)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Aux angle-----------------------\n",
    "dif = torch.abs(finalpred1.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred1.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------last fc angle-------------------------\n",
    "dif = torch.abs(finalpred2.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred2.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "MSElossNor==0.0006595446894091121 ABSlossNor==0.01841172655423482 RELlossNor0.011740146730329191\n",
      "MSEloss==0.21105430061091587 ABSloss==5.891752497355142 RELloss3.7568469537053413\n"
     ]
    }
   ],
   "source": [
    "#--------------------Aux distance-----------------------\n",
    "dif = torch.abs(finalpred1.data[:,0]-ytestT[:,1,0])\n",
    "dif1 = torch.abs((finalpred1.data[:,0]-ytestT[:,1,0])/ytestT[:,1,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossNor = torch.mean(torch.pow(dif,2))\n",
    "ABSlossNor = torch.mean(dif)\n",
    "RELlossNor = torch.mean(dif1)\n",
    "MSEloss = MSElossNor*320\n",
    "ABSloss = ABSlossNor*320\n",
    "RELloss = RELlossNor*320\n",
    "print(\"MSElossNor==\"+str(MSElossNor),\"ABSlossNor==\"+str(ABSlossNor),\"RELlossNor\"+str(RELlossNor))\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss),\"RELloss\"+str(RELloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "MSElossNor==0.0002716392945914473 ABSlossNor==0.012936254739761352 RELlossNor0.008276351379272455\n",
      "MSEloss==0.08692457426926314 ABSloss==4.139601516723633 RELloss2.6484324413671856\n"
     ]
    }
   ],
   "source": [
    "#----------------------last fc distance-------------------------\n",
    "dif = torch.abs(finalpred2.data[:,0]-ytestT[:,1,0])\n",
    "dif1 = torch.abs((finalpred2.data[:,0]-ytestT[:,1,0])/ytestT[:,1,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossNor = torch.mean(torch.pow(dif,2))\n",
    "ABSlossNor = torch.mean(dif)\n",
    "RELlossNor = torch.mean(dif1)\n",
    "MSEloss = MSElossNor*320\n",
    "ABSloss = ABSlossNor*320\n",
    "RELloss = RELlossNor*320\n",
    "print(\"MSElossNor==\"+str(MSElossNor),\"ABSlossNor==\"+str(ABSlossNor),\"RELlossNor\"+str(RELlossNor))\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss),\"RELloss\"+str(RELloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027310848236083984\n"
     ]
    }
   ],
   "source": [
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "tic=time.time()\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "intr=time.time()-tic\n",
    "print(intr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
