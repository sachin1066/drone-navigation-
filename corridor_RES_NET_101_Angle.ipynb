{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvrlab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net_101 = models.resnet101(pretrained=True)\n",
    "#dense161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_net_101= nn.Sequential(*list(res_net_101.children())[:-2])\n",
    "res_net_101\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input=Variable(torch.randn(1,3,180,320))\n",
    "# output=res_net_101(input)\n",
    "# print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.bias.data.zero_()    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Sequential(nn.BatchNorm2d(2048),nn.ReLU(),nn.Conv2d(2048,1024,1))\n",
    "conv1 = InitializeWeights(conv1)\n",
    "conv2 = nn.Sequential(nn.BatchNorm2d(1024),nn.ReLU(),nn.Conv2d(1024,128,5))\n",
    "conv2 = InitializeWeights(conv2)\n",
    "conv3 = nn.Sequential(nn.BatchNorm2d(128),nn.ReLU(),nn.Conv2d(128,8,1))\n",
    "conv3 = InitializeWeights(conv3)\n",
    "norm1 = nn.BatchNorm2d(8)\n",
    "norm1 = InitializeWeights(norm1)\n",
    "fc1 = nn.Linear(96, 1)\n",
    "fc1 = InitializeWeights(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel4(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyModel4, self).__init__()\n",
    "        self.pretrained_model = nn.Sequential(*list(res_net_101.children())[:-2])\n",
    "        self.conv1 = conv1\n",
    "        self.conv2 = conv2\n",
    "        self.conv3 = conv3\n",
    "        self.norm1 = norm1\n",
    "        self.fc1 = fc1\n",
    "   \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm1(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #print(x.size())\n",
    "        #x = self.conv4(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModel4(res_net_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5180251598358154\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "input=Variable(torch.randn(1,3,180,320))\n",
    "tic=time.time()\n",
    "output=net(input)\n",
    "tac=time.time()\n",
    "print(tac-tic)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  328\n",
      "Total number of parameters --->  44549160\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in res_net_101.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(5, 3, 180, 320))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.282239198684692\n"
     ]
    }
   ],
   "source": [
    "tic_1=time.time()\n",
    "file = h5py.File('./DATASET/CODE/NewTrainData_21000_distance.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)\n",
    "toc_1=time.time()\n",
    "print(toc_1-tic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData_random_distance_1.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#print(xtrainT.size(), xtestT.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "#xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#xtestT = torch.div(xtestT,255.0)\n",
    "#mn = [0.406,0.456,0.485]\n",
    "#sd = [0.225,0.224,0.229]\n",
    "#norm = Normalize(mn,sd)\n",
    "#xtestT = norm(xtestT)\n",
    "#print(xtestT.size(), ytestT.size())\n",
    "#print(torch.min(xtestT), torch.max(xtestT),torch.min(ytestT), torch.max(ytestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = batch_rgb_to_bgr(x)\n",
    "    x = torch.div(x,255.0)\n",
    "    mn = [0.406,0.456,0.485]\n",
    "    sd = [0.225,0.224,0.229]\n",
    "    x[:,0,:,:] = (x[:,0,:,:]-mn[0])/sd[0]\n",
    "    x[:,1,:,:] = (x[:,1,:,:]-mn[1])/sd[1]\n",
    "    x[:,2,:,:] = (x[:,2,:,:]-mn[2])/sd[2]\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff = torch.abs(tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "        loss = torch.mean(diff)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BerhuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BerhuLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        mt = tar[:,0]\n",
    "        mp = inp[:,0]\n",
    "        diff = torch.abs(mt-mp)        \n",
    "        lossval = 0.0        \n",
    "        c = 0.2 * torch.max(diff)\n",
    "        l1 = torch.mean(diff)\n",
    "        l2 = torch.mean(torch.pow(diff,2))\n",
    "        if l1 <= c:\n",
    "            lossval = l1\n",
    "        else:\n",
    "            lossval = (l2+c**2)/(2*c)\n",
    "        \n",
    "        return lossval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = torch.FloatTensor(ytrainT[5,0])\n",
    "alpha = ytrainT[5,0]\n",
    "#print(alpha.shape)\n",
    "xt = torch.FloatTensor([np.cos(alpha),np.sin(alpha)])\n",
    "print(ytrainT[5,0],xt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ytrainT[5:10,0]\n",
    "print(torch.cos(alpha[0:1]-alpha[1:2]))\n",
    "xt = torch.stack([torch.cos(alpha[0:1]),torch.sin(alpha[0:1])])\n",
    "xp = torch.stack([torch.cos(alpha[1:2]),torch.sin(alpha[1:2])])\n",
    "print(xt[0],xt[1])\n",
    "#print(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar,batch_sz):\n",
    "        alpha_t = tar[:,0]\n",
    "        alpha_p = inp[:,0]\n",
    "        #xt = torch.stack([torch.cos(alpha_t),torch.sin(alpha_t)])\n",
    "        #xp = torch.stack([torch.cos(alpha_p),torch.sin(alpha_p)])\n",
    "        #cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        #loss = cos(xt, xp)\n",
    "        #return loss\n",
    "        loss = Variable(torch.FloatTensor(batch_sz).zero_(), requires_grad=False).cuda()\n",
    "        for i in range(batch_sz):          \n",
    "            loss[i] = torch.cos(alpha_t[i:i+1]-alpha_p[i:i+1])\n",
    "            \n",
    "        lossval = 1.0-torch.mean(loss)    \n",
    "        #print(lossval)\n",
    "        return lossval\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.03160069 at epoch 1 completed in 3m 53s\n",
      "save the weights\n",
      "Loss = 0.02792677 at epoch 2 completed in 3m 54s\n",
      "Loss 0.02821031718736603 is bigger than Loss 0.02792677031563861 in the prev epoch \n",
      "Loss = 0.02821032 at epoch 3 completed in 3m 53s\n",
      "save the weights\n",
      "Loss = 0.02356691 at epoch 4 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02144805 at epoch 5 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.02060356 at epoch 6 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.02007288 at epoch 7 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01793856 at epoch 8 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01676911 at epoch 9 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01608066 at epoch 10 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01418320 at epoch 11 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01366430 at epoch 12 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01330893 at epoch 13 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01263719 at epoch 14 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01130252 at epoch 15 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01079282 at epoch 16 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.01008053 at epoch 17 completed in 3m 43s\n",
      "Loss 0.010598790903708757 is bigger than Loss 0.010080534199341417 in the prev epoch \n",
      "Loss = 0.01059879 at epoch 18 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00954549 at epoch 19 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00839195 at epoch 20 completed in 3m 43s\n",
      "Loss 0.008624340535718068 is bigger than Loss 0.008391948747670372 in the prev epoch \n",
      "Loss = 0.00862434 at epoch 21 completed in 3m 43s\n",
      "Loss 0.008641156180806103 is bigger than Loss 0.008391948747670372 in the prev epoch \n",
      "Loss = 0.00864116 at epoch 22 completed in 3m 43s\n",
      "Loss 0.008523995611107069 is bigger than Loss 0.008391948747670372 in the prev epoch \n",
      "Loss = 0.00852400 at epoch 23 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00806888 at epoch 24 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00757037 at epoch 25 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00710308 at epoch 26 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00676128 at epoch 27 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00644541 at epoch 28 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00617819 at epoch 29 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00579054 at epoch 30 completed in 3m 43s\n",
      "Loss 0.0058033189510128345 is bigger than Loss 0.005790536805898654 in the prev epoch \n",
      "Loss = 0.00580332 at epoch 31 completed in 3m 43s\n",
      "Loss 0.006302720174814265 is bigger than Loss 0.005790536805898654 in the prev epoch \n",
      "Loss = 0.00630272 at epoch 32 completed in 3m 43s\n",
      "Loss 0.005824364406012352 is bigger than Loss 0.005790536805898654 in the prev epoch \n",
      "Loss = 0.00582436 at epoch 33 completed in 3m 43s\n",
      "Loss 0.005904705763484038 is bigger than Loss 0.005790536805898654 in the prev epoch \n",
      "Loss = 0.00590471 at epoch 34 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00548893 at epoch 35 completed in 3m 43s\n",
      "Loss 0.005515776858443304 is bigger than Loss 0.0054889308248779614 in the prev epoch \n",
      "Loss = 0.00551578 at epoch 36 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00505495 at epoch 37 completed in 3m 43s\n",
      "Loss 0.005167199533344026 is bigger than Loss 0.005054948727289835 in the prev epoch \n",
      "Loss = 0.00516720 at epoch 38 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00495954 at epoch 39 completed in 3m 43s\n",
      "Loss 0.005174166977050755 is bigger than Loss 0.004959535179216243 in the prev epoch \n",
      "Loss = 0.00517417 at epoch 40 completed in 3m 43s\n",
      "Loss 0.005015067019987674 is bigger than Loss 0.004959535179216243 in the prev epoch \n",
      "Loss = 0.00501507 at epoch 41 completed in 3m 43s\n",
      "Loss 0.005278878778307921 is bigger than Loss 0.004959535179216243 in the prev epoch \n",
      "Loss = 0.00527888 at epoch 42 completed in 3m 43s\n",
      "Loss 0.005188948466398178 is bigger than Loss 0.004959535179216243 in the prev epoch \n",
      "Loss = 0.00518895 at epoch 43 completed in 3m 43s\n",
      "Learning rate changed from 0.002 to 0.0004\n",
      "Loss 0.005094941509887576 is bigger than Loss 0.004959535179216243 in the prev epoch \n",
      "Loss = 0.00509494 at epoch 44 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00288707 at epoch 45 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00277619 at epoch 46 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00274263 at epoch 47 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00266334 at epoch 48 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00261864 at epoch 49 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00260292 at epoch 50 completed in 3m 43s\n",
      "Loss 0.0026390589764785193 is bigger than Loss 0.002602920926264708 in the prev epoch \n",
      "Loss = 0.00263906 at epoch 51 completed in 3m 43s\n",
      "Loss 0.002612222450607944 is bigger than Loss 0.002602920926264708 in the prev epoch \n",
      "Loss = 0.00261222 at epoch 52 completed in 3m 43s\n",
      "Loss 0.002643546462280765 is bigger than Loss 0.002602920926264708 in the prev epoch \n",
      "Loss = 0.00264355 at epoch 53 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00258170 at epoch 54 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00257776 at epoch 55 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00252931 at epoch 56 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00241286 at epoch 57 completed in 3m 43s\n",
      "Loss 0.002425031444456959 is bigger than Loss 0.0024128562773001344 in the prev epoch \n",
      "Loss = 0.00242503 at epoch 58 completed in 3m 43s\n",
      "Loss 0.00256647133157544 is bigger than Loss 0.0024128562773001344 in the prev epoch \n",
      "Loss = 0.00256647 at epoch 59 completed in 3m 43s\n",
      "Loss 0.0024953791767447476 is bigger than Loss 0.0024128562773001344 in the prev epoch \n",
      "Loss = 0.00249538 at epoch 60 completed in 3m 43s\n",
      "Loss 0.0025269618823326083 is bigger than Loss 0.0024128562773001344 in the prev epoch \n",
      "Loss = 0.00252696 at epoch 61 completed in 3m 43s\n",
      "Learning rate changed from 0.0004 to 8e-05\n",
      "Loss 0.0024681233124630064 is bigger than Loss 0.0024128562773001344 in the prev epoch \n",
      "Loss = 0.00246812 at epoch 62 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00210661 at epoch 63 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00205243 at epoch 64 completed in 3m 43s\n",
      "Loss 0.0020667069347664008 is bigger than Loss 0.0020524269823605818 in the prev epoch \n",
      "Loss = 0.00206671 at epoch 65 completed in 3m 43s\n",
      "Loss 0.0020697735540480115 is bigger than Loss 0.0020524269823605818 in the prev epoch \n",
      "Loss = 0.00206977 at epoch 66 completed in 3m 43s\n",
      "Loss 0.002067612989894338 is bigger than Loss 0.0020524269823605818 in the prev epoch \n",
      "Loss = 0.00206761 at epoch 67 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00199937 at epoch 68 completed in 3m 43s\n",
      "Loss 0.0020073856581889447 is bigger than Loss 0.001999367241570286 in the prev epoch \n",
      "Loss = 0.00200739 at epoch 69 completed in 3m 43s\n",
      "Loss 0.0020185541111000234 is bigger than Loss 0.001999367241570286 in the prev epoch \n",
      "Loss = 0.00201855 at epoch 70 completed in 3m 43s\n",
      "Loss 0.0020624529510470378 is bigger than Loss 0.001999367241570286 in the prev epoch \n",
      "Loss = 0.00206245 at epoch 71 completed in 3m 43s\n",
      "Loss 0.002056526786736431 is bigger than Loss 0.001999367241570286 in the prev epoch \n",
      "Loss = 0.00205653 at epoch 72 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00195999 at epoch 73 completed in 3m 43s\n",
      "Loss 0.002048495991510294 is bigger than Loss 0.0019599883528869768 in the prev epoch \n",
      "Loss = 0.00204850 at epoch 74 completed in 3m 43s\n",
      "Loss 0.00204203998293018 is bigger than Loss 0.0019599883528869768 in the prev epoch \n",
      "Loss = 0.00204204 at epoch 75 completed in 3m 43s\n",
      "Loss 0.0020016193988599946 is bigger than Loss 0.0019599883528869768 in the prev epoch \n",
      "Loss = 0.00200162 at epoch 76 completed in 3m 43s\n",
      "Loss 0.0020383566396222227 is bigger than Loss 0.0019599883528869768 in the prev epoch \n",
      "Loss = 0.00203836 at epoch 77 completed in 3m 43s\n",
      "Learning rate changed from 8e-05 to 1.6000000000000003e-05\n",
      "Loss 0.001990889670282958 is bigger than Loss 0.0019599883528869768 in the prev epoch \n",
      "Loss = 0.00199089 at epoch 78 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00192174 at epoch 79 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00192041 at epoch 80 completed in 3m 43s\n",
      "Loss 0.0019223497054051786 is bigger than Loss 0.0019204074328410484 in the prev epoch \n",
      "Loss = 0.00192235 at epoch 81 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00191713 at epoch 82 completed in 3m 43s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0019424923567012645 is bigger than Loss 0.001917128679342568 in the prev epoch \n",
      "Loss = 0.00194249 at epoch 83 completed in 3m 43s\n",
      "Loss 0.0019256775906043384 is bigger than Loss 0.001917128679342568 in the prev epoch \n",
      "Loss = 0.00192568 at epoch 84 completed in 3m 43s\n",
      "Loss 0.0019308834542919478 is bigger than Loss 0.001917128679342568 in the prev epoch \n",
      "Loss = 0.00193088 at epoch 85 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00190481 at epoch 86 completed in 3m 43s\n",
      "Loss 0.0019265794472414113 is bigger than Loss 0.001904814022460154 in the prev epoch \n",
      "Loss = 0.00192658 at epoch 87 completed in 3m 43s\n",
      "Loss 0.0019247294416917223 is bigger than Loss 0.001904814022460154 in the prev epoch \n",
      "Loss = 0.00192473 at epoch 88 completed in 3m 43s\n",
      "Loss 0.0019496920261354673 is bigger than Loss 0.001904814022460154 in the prev epoch \n",
      "Loss = 0.00194969 at epoch 89 completed in 3m 43s\n",
      "Loss 0.0019155199652803796 is bigger than Loss 0.001904814022460154 in the prev epoch \n",
      "Loss = 0.00191552 at epoch 90 completed in 3m 43s\n",
      "Learning rate changed from 1.6000000000000003e-05 to 3.2000000000000007e-06\n",
      "Loss 0.0019061009521551785 is bigger than Loss 0.001904814022460154 in the prev epoch \n",
      "Loss = 0.00190610 at epoch 91 completed in 3m 43s\n",
      "Loss 0.001906034570364725 is bigger than Loss 0.001904814022460154 in the prev epoch \n",
      "Loss = 0.00190603 at epoch 92 completed in 3m 43s\n",
      "Loss 0.0019326747831932855 is bigger than Loss 0.001904814022460154 in the prev epoch \n",
      "Loss = 0.00193267 at epoch 93 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00188454 at epoch 94 completed in 3m 43s\n",
      "save the weights\n",
      "Loss = 0.00186668 at epoch 95 completed in 3m 43s\n",
      "Loss 0.0018820179238294562 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00188202 at epoch 96 completed in 3m 43s\n",
      "Loss 0.0019082746936363124 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00190827 at epoch 97 completed in 3m 43s\n",
      "Loss 0.0019229856297551162 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00192299 at epoch 98 completed in 3m 43s\n",
      "Loss 0.0018785278625520212 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00187853 at epoch 99 completed in 3m 43s\n",
      "Learning rate changed from 3.2000000000000007e-06 to 6.400000000000001e-07\n",
      "Loss 0.0019123422066193252 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00191234 at epoch 100 completed in 3m 43s\n",
      "Loss 0.001914242615125009 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00191424 at epoch 101 completed in 3m 43s\n",
      "Loss 0.0019141261138775874 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00191413 at epoch 102 completed in 3m 43s\n",
      "Loss 0.0019071396883754504 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00190714 at epoch 103 completed in 3m 43s\n",
      "Loss 0.0019242257037244382 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00192423 at epoch 104 completed in 3m 43s\n",
      "Learning rate changed from 6.400000000000001e-07 to 1.2800000000000003e-07\n",
      "Loss 0.0019335182636444057 is bigger than Loss 0.0018666798694591438 in the prev epoch \n",
      "Loss = 0.00193352 at epoch 105 completed in 3m 43s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=.002, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = CosineLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss =float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 105, 21000, 0, 40, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,1)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind,1,0]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/CustomLoss_new/RES_NET_101_CustomLoss_new_21000_DISTANCE_105_epochs.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainT = xtrainT.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2800000000000003e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/CustomLoss_new/RES_NET_101_CustomLoss_new_21000_DISTANCE_105_epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([300, 1])\n",
      "num of batches ---> 15\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 20\n",
    "n_examples = 300\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred[start:end] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "MSElossRad==1.2399078823129337 ABSlossRad==1.0826517504453659 RELlossRad0.6899038264155388\n",
      "MSElossDeg==71.04148864153467 ABSlossDeg==62.03137598297031 RELlossDeg39.52857752353653\n"
     ]
    }
   ],
   "source": [
    "#------------------------Angle----------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "MSElossNor==0.00037076952618553114 ABSlossNor==0.013175938725471497 RELlossNor0.04587706564504818\n",
      "MSEloss==0.11864624837936996 ABSloss==4.216300392150879 RELloss14.680661006415418\n"
     ]
    }
   ],
   "source": [
    "#---------------DISTANCE------------------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,1,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,1,0])/ytestT[:,1,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSElossNor = torch.mean(torch.pow(dif,2))\n",
    "ABSlossNor = torch.mean(dif)\n",
    "RELlossNor = torch.mean(dif1)\n",
    "MSEloss = MSElossNor*320\n",
    "ABSloss = ABSlossNor*320\n",
    "RELloss = RELlossNor*320\n",
    "print(\"MSElossNor==\"+str(MSElossNor),\"ABSlossNor==\"+str(ABSlossNor),\"RELlossNor\"+str(RELlossNor))\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss),\"RELloss\"+str(RELloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026737689971923828\n"
     ]
    }
   ],
   "source": [
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "tic=time.time()\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "intr=time.time()-tic\n",
    "print(intr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalpred.size())\n",
    "print(ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0,0]*(180/np.pi)))\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Pred angle===\"+str(finalpred.data[ind,0]*(180/np.pi)))\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "#print(finalpred.data[n,0]*(180/np.pi))\n",
    "#print(ytestT[ind,0,0]*(180/np.pi))\n",
    "print(ytestT[ind,0,0]*(180/np.pi)-finalpred.data[ind,0]*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ABSlossRad*(180/np.pi))\n",
    "print(MSEloss*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(ytestT[:,0]*(180/np.pi))\n",
    "#print(ytestT[:,0,2]*(180/np.pi))\n",
    "a = ytestT[:,0,0]*(180/np.pi)\n",
    "print(a.size())\n",
    "np.savetxt(\"test.csv\", a.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "#np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalpred.data[:,0]*(180/np.pi))\n",
    "print(finalpred.data[:,0]*(180/np.pi))\n",
    "b = finalpred.data[:,0]*(180/np.pi)\n",
    "c=torch.abs(ytestT[:,0,0]*(180/np.pi)- finalpred.data[:,0]*(180/np.pi))\n",
    "np.savetxt(\"pred.csv\", b.numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"diff.csv\", c.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred.data[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "test = cv2.imread(\"./Test_Net_image/4.JPG\")\n",
    "print(test.shape)\n",
    "test = imresize(test,(180,320,3))\n",
    "#imshow(test)\n",
    "test = test.transpose(2,0,1)\n",
    "test = np.reshape(test,(1,3,180,320))\n",
    "test = test.astype(np.float32)\n",
    "testPT = torch.from_numpy(test).float()\n",
    "testPT = batch_rgb_to_bgr(testPT)\n",
    "testPT = torch.div(testPT,255.0)\n",
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "testPT = norm(testPT)\n",
    "'''\n",
    "ind = 2000\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0]*(180/np.pi)), ytestT[ind,0])\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "#'''\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Angle===\"+str(test_pred.data[0,0]*(180/np.pi)), test_pred.data[0,0])\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
