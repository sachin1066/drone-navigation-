{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "from math import log,sqrt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corridor (\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(7, 7), stride=(2, 2))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (conv3): Conv2d(16, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear (640 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      "  (fc4): Linear (10 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Corridor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Corridor, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = InitializeWeights(nn.Conv2d(3, 6, 7, 2))\n",
    "        self.conv2 = InitializeWeights(nn.Conv2d(6, 16, 5, 2))\n",
    "        self.conv3 = InitializeWeights(nn.Conv2d(16, 20, 3, 1))\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(20 * 4 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Corridor()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "torch.Size([6, 3, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  14\n",
      "Total number of parameters --->  94149\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in net.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1926\n",
      " 0.1944\n",
      " 0.1874\n",
      " 0.1889\n",
      " 0.1940\n",
      "[torch.FloatTensor of size 5x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(5, 3, 180, 320))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTrainData.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5670, 3, 180, 320]) torch.Size([912, 3, 180, 320])\n"
     ]
    }
   ],
   "source": [
    "xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "print(xtrainT.size(), xtestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5670, 3, 180, 320]) torch.Size([5670, 2]) torch.Size([912, 3, 180, 320]) torch.Size([912, 2])\n"
     ]
    }
   ],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1007792949676514 2.640000104904175 -1.8267831802368164 2.640000104904175\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff = torch.abs(tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "        loss = torch.mean(torch.pow(diff,2))\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.0082 at epoch 1 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 2 completed in 0m 3s\n",
      "Loss 0.00817682746139961 is bigger than Loss 0.008175692417555384 in the prev epoch \n",
      "Loss = 0.0082 at epoch 3 completed in 0m 3s\n",
      "Loss 0.008176010004096913 is bigger than Loss 0.008175692417555384 in the prev epoch \n",
      "Loss = 0.0082 at epoch 4 completed in 0m 3s\n",
      "Loss 0.00817585787144564 is bigger than Loss 0.008175692417555384 in the prev epoch \n",
      "Loss = 0.0082 at epoch 5 completed in 0m 3s\n",
      "Loss 0.008176212542734682 is bigger than Loss 0.008175692417555384 in the prev epoch \n",
      "Loss = 0.0082 at epoch 6 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 7 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 8 completed in 0m 3s\n",
      "Loss 0.008175498103752496 is bigger than Loss 0.00817520756252287 in the prev epoch \n",
      "Loss = 0.0082 at epoch 9 completed in 0m 3s\n",
      "Loss 0.008175935978735132 is bigger than Loss 0.00817520756252287 in the prev epoch \n",
      "Loss = 0.0082 at epoch 10 completed in 0m 3s\n",
      "Loss 0.008176471764547015 is bigger than Loss 0.00817520756252287 in the prev epoch \n",
      "Loss = 0.0082 at epoch 11 completed in 0m 3s\n",
      "Loss 0.008176302079052199 is bigger than Loss 0.00817520756252287 in the prev epoch \n",
      "Loss = 0.0082 at epoch 12 completed in 0m 3s\n",
      "Learning rate changed from 1.6000000000000003e-07 to 3.200000000000001e-08\n",
      "Loss 0.008175559371473294 is bigger than Loss 0.00817520756252287 in the prev epoch \n",
      "Loss = 0.0082 at epoch 13 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 14 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 15 completed in 0m 3s\n",
      "Loss 0.008173161703679297 is bigger than Loss 0.008173068807948203 in the prev epoch \n",
      "Loss = 0.0082 at epoch 16 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 17 completed in 0m 3s\n",
      "Loss 0.008172991888834127 is bigger than Loss 0.008172988145065209 in the prev epoch \n",
      "Loss = 0.0082 at epoch 18 completed in 0m 3s\n",
      "Loss 0.008173169841664647 is bigger than Loss 0.008172988145065209 in the prev epoch \n",
      "Loss = 0.0082 at epoch 19 completed in 0m 3s\n",
      "Loss 0.00817313111047186 is bigger than Loss 0.008172988145065209 in the prev epoch \n",
      "Loss = 0.0082 at epoch 20 completed in 0m 3s\n",
      "Loss 0.008173128501290374 is bigger than Loss 0.008172988145065209 in the prev epoch \n",
      "Loss = 0.0082 at epoch 21 completed in 0m 3s\n",
      "Learning rate changed from 3.200000000000001e-08 to 6.400000000000002e-09\n",
      "Loss 0.008173033330840844 is bigger than Loss 0.008172988145065209 in the prev epoch \n",
      "Loss = 0.0082 at epoch 22 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 23 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 24 completed in 0m 3s\n",
      "Loss 0.008172657908058717 is bigger than Loss 0.008172652573280394 in the prev epoch \n",
      "Loss = 0.0082 at epoch 25 completed in 0m 3s\n",
      "Loss 0.008172655341378043 is bigger than Loss 0.008172652573280394 in the prev epoch \n",
      "Loss = 0.0082 at epoch 26 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 27 completed in 0m 3s\n",
      "Loss 0.008172654472882781 is bigger than Loss 0.008172651782395344 in the prev epoch \n",
      "Loss = 0.0082 at epoch 28 completed in 0m 3s\n",
      "Loss 0.008172652944700706 is bigger than Loss 0.008172651782395344 in the prev epoch \n",
      "Loss = 0.0082 at epoch 29 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 30 completed in 0m 3s\n",
      "Loss 0.008172660393433439 is bigger than Loss 0.008172650167363736 in the prev epoch \n",
      "Loss = 0.0082 at epoch 31 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 32 completed in 0m 3s\n",
      "Loss 0.008172641251428376 is bigger than Loss 0.008172632649629596 in the prev epoch \n",
      "Loss = 0.0082 at epoch 33 completed in 0m 3s\n",
      "Loss 0.008172659506459559 is bigger than Loss 0.008172632649629596 in the prev epoch \n",
      "Loss = 0.0082 at epoch 34 completed in 0m 3s\n",
      "Loss 0.008172648001669178 is bigger than Loss 0.008172632649629596 in the prev epoch \n",
      "Loss = 0.0082 at epoch 35 completed in 0m 3s\n",
      "Loss 0.00817264212546723 is bigger than Loss 0.008172632649629596 in the prev epoch \n",
      "Loss = 0.0082 at epoch 36 completed in 0m 3s\n",
      "Learning rate changed from 6.400000000000002e-09 to 1.2800000000000003e-09\n",
      "Loss 0.008172641726328981 is bigger than Loss 0.008172632649629596 in the prev epoch \n",
      "Loss = 0.0082 at epoch 37 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 38 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 39 completed in 0m 3s\n",
      "Loss 0.008172614163615626 is bigger than Loss 0.008172613594474064 in the prev epoch \n",
      "Loss = 0.0082 at epoch 40 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 41 completed in 0m 3s\n",
      "Loss 0.008172613472515158 is bigger than Loss 0.00817261345773226 in the prev epoch \n",
      "Loss = 0.0082 at epoch 42 completed in 0m 3s\n",
      "Loss 0.008172614067526805 is bigger than Loss 0.00817261345773226 in the prev epoch \n",
      "Loss = 0.0082 at epoch 43 completed in 0m 3s\n",
      "Loss 0.008172614278183093 is bigger than Loss 0.00817261345773226 in the prev epoch \n",
      "Loss = 0.0082 at epoch 44 completed in 0m 3s\n",
      "Loss 0.008172614189485712 is bigger than Loss 0.00817261345773226 in the prev epoch \n",
      "Loss = 0.0082 at epoch 45 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 46 completed in 0m 3s\n",
      "Loss 0.008172614309596754 is bigger than Loss 0.008172613295120382 in the prev epoch \n",
      "Loss = 0.0082 at epoch 47 completed in 0m 3s\n",
      "Loss 0.00817261414883274 is bigger than Loss 0.008172613295120382 in the prev epoch \n",
      "Loss = 0.0082 at epoch 48 completed in 0m 3s\n",
      "Loss 0.008172613561212539 is bigger than Loss 0.008172613295120382 in the prev epoch \n",
      "Loss = 0.0082 at epoch 49 completed in 0m 3s\n",
      "Loss 0.0081726135094724 is bigger than Loss 0.008172613295120382 in the prev epoch \n",
      "Loss = 0.0082 at epoch 50 completed in 0m 3s\n",
      "Learning rate changed from 1.2800000000000003e-09 to 2.5600000000000005e-10\n",
      "Loss 0.00817261586734463 is bigger than Loss 0.008172613295120382 in the prev epoch \n",
      "Loss = 0.0082 at epoch 51 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 52 completed in 0m 3s\n",
      "Loss 0.008172612969896626 is bigger than Loss 0.00817261279619757 in the prev epoch \n",
      "Loss = 0.0082 at epoch 53 completed in 0m 3s\n",
      "Loss 0.008172612831306955 is bigger than Loss 0.00817261279619757 in the prev epoch \n",
      "Loss = 0.0082 at epoch 54 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 55 completed in 0m 3s\n",
      "save the weights\n",
      "Loss = 0.0082 at epoch 56 completed in 0m 3s\n",
      "Loss 0.008172612966200902 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 57 completed in 0m 3s\n",
      "Loss 0.008172612958809453 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 58 completed in 0m 3s\n",
      "Loss 0.008172612905221444 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 59 completed in 0m 3s\n",
      "Loss 0.00817261290152573 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 60 completed in 0m 3s\n",
      "Learning rate changed from 2.5600000000000005e-10 to 5.120000000000001e-11\n",
      "Loss 0.008172613136204218 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 61 completed in 0m 3s\n",
      "Loss 0.008172612955113722 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 62 completed in 0m 3s\n",
      "Loss 0.008172613005006012 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 63 completed in 0m 3s\n",
      "Loss 0.008172612951418004 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 64 completed in 0m 3s\n",
      "Loss 0.0081726131620743 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 65 completed in 0m 3s\n",
      "Learning rate changed from 5.120000000000001e-11 to 1.0240000000000002e-11\n",
      "Loss 0.008172613036419666 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 66 completed in 0m 3s\n",
      "Loss 0.008172613034571808 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 67 completed in 0m 3s\n",
      "Loss 0.008172613114029878 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 68 completed in 0m 3s\n",
      "Loss 0.008172613080768358 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 69 completed in 0m 3s\n",
      "Loss 0.00817261315098712 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 70 completed in 0m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate changed from 1.0240000000000002e-11 to 2.048e-12\n",
      "Loss 0.008172613143595672 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 71 completed in 0m 3s\n",
      "Loss 0.00817261305489829 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 72 completed in 0m 3s\n",
      "Loss 0.00817261303272394 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 73 completed in 0m 3s\n",
      "Loss 0.00817261309555126 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 74 completed in 0m 3s\n",
      "Loss 0.00817261304011539 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 75 completed in 0m 3s\n",
      "Learning rate changed from 2.048e-12 to 4.0960000000000004e-13\n",
      "Loss 0.00817261308815981 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 76 completed in 0m 3s\n",
      "Loss 0.008172613125117056 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 77 completed in 0m 3s\n",
      "Loss 0.008172612966200895 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 78 completed in 0m 3s\n",
      "Loss 0.008172613110334154 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 79 completed in 0m 3s\n",
      "Loss 0.008172612979135931 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 80 completed in 0m 3s\n",
      "Learning rate changed from 4.0960000000000004e-13 to 8.192e-14\n",
      "Loss 0.00817261296250517 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 81 completed in 0m 3s\n",
      "Loss 0.008172612955113729 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 82 completed in 0m 3s\n",
      "Loss 0.008172613099246977 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 83 completed in 0m 3s\n",
      "Loss 0.008172613077072633 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 84 completed in 0m 3s\n",
      "Loss 0.008172613006853876 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 85 completed in 0m 3s\n",
      "Learning rate changed from 8.192e-14 to 1.6384e-14\n",
      "Loss 0.008172613080768354 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 86 completed in 0m 3s\n",
      "Loss 0.008172613054898284 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 87 completed in 0m 3s\n",
      "Loss 0.008172613143595674 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 88 completed in 0m 3s\n",
      "Loss 0.00817261302533249 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 89 completed in 0m 3s\n",
      "Loss 0.008172613008701734 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 90 completed in 0m 3s\n",
      "Learning rate changed from 1.6384e-14 to 3.2768000000000003e-15\n",
      "Loss 0.008172613032723937 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 91 completed in 0m 3s\n",
      "Loss 0.00817261292000435 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 92 completed in 0m 3s\n",
      "Loss 0.008172612995766698 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 93 completed in 0m 3s\n",
      "Loss 0.008172612951418001 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 94 completed in 0m 3s\n",
      "Loss 0.008172613065985466 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 95 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 3.2768000000000003e-15 to 1e-15\n",
      "Loss 0.008172613162074299 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 96 completed in 0m 3s\n",
      "Loss 0.008172613077072633 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 97 completed in 0m 3s\n",
      "Loss 0.008172613069681185 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 98 completed in 0m 3s\n",
      "Loss 0.008172613091855532 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 99 completed in 0m 3s\n",
      "Loss 0.008172613017941033 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 100 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172612992070966 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 101 completed in 0m 3s\n",
      "Loss 0.008172613029028214 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 102 completed in 0m 3s\n",
      "Loss 0.008172613136204229 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 103 completed in 0m 3s\n",
      "Loss 0.008172613080768353 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 104 completed in 0m 3s\n",
      "Loss 0.008172612995766704 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 105 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613054898279 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 106 completed in 0m 3s\n",
      "Loss 0.008172613010549596 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 107 completed in 0m 3s\n",
      "Loss 0.008172613114029878 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 108 completed in 0m 3s\n",
      "Loss 0.008172613058594009 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 109 completed in 0m 3s\n",
      "Loss 0.008172613029028223 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 110 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172612881199231 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 111 completed in 0m 3s\n",
      "Loss 0.008172613032723937 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 112 completed in 0m 3s\n",
      "Loss 0.008172613054898288 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 113 completed in 0m 3s\n",
      "Loss 0.00817261315837857 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 114 completed in 0m 3s\n",
      "Loss 0.008172613054898288 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 115 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613025332487 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 116 completed in 0m 3s\n",
      "Loss 0.008172612969896619 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 117 completed in 0m 3s\n",
      "Loss 0.008172612969896619 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 118 completed in 0m 3s\n",
      "Loss 0.008172613043811117 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 119 completed in 0m 3s\n",
      "Loss 0.008172613099246977 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 120 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.00817261306598546 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 121 completed in 0m 3s\n",
      "Loss 0.008172612992070966 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 122 completed in 0m 3s\n",
      "Loss 0.008172612988375245 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 123 completed in 0m 3s\n",
      "Loss 0.008172612951418 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 124 completed in 0m 3s\n",
      "Loss 0.008172612986527389 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 125 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172612951418 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 126 completed in 0m 3s\n",
      "Loss 0.008172613080768354 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 127 completed in 0m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.008172613171313604 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 128 completed in 0m 3s\n",
      "Loss 0.008172612973592348 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 129 completed in 0m 3s\n",
      "Loss 0.008172613047506843 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 130 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613062289733 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 131 completed in 0m 3s\n",
      "Loss 0.008172613126964915 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 132 completed in 0m 3s\n",
      "Loss 0.008172613051202567 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 133 completed in 0m 3s\n",
      "Loss 0.00817261305120256 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 134 completed in 0m 3s\n",
      "Loss 0.008172613088159815 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 135 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613132508508 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 136 completed in 0m 3s\n",
      "Loss 0.008172613021636776 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 137 completed in 0m 3s\n",
      "Loss 0.008172613136204225 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 138 completed in 0m 3s\n",
      "Loss 0.008172613121421327 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 139 completed in 0m 3s\n",
      "Loss 0.008172612888590678 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 140 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613195335814 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 141 completed in 0m 3s\n",
      "Loss 0.008172613121421325 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 142 completed in 0m 3s\n",
      "Loss 0.008172612956961587 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 143 completed in 0m 3s\n",
      "Loss 0.008172613187944372 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 144 completed in 0m 3s\n",
      "Loss 0.008172613003158144 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 145 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613016093183 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 146 completed in 0m 3s\n",
      "Loss 0.008172612979135931 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 147 completed in 0m 3s\n",
      "Loss 0.008172613091855525 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 148 completed in 0m 3s\n",
      "Loss 0.008172613065985464 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 149 completed in 0m 3s\n",
      "Loss 0.008172612923700076 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 150 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613040115389 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 151 completed in 0m 3s\n",
      "Loss 0.008172613095551253 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 152 completed in 0m 3s\n",
      "Loss 0.008172613088159801 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 153 completed in 0m 3s\n",
      "Loss 0.00817261298467952 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 154 completed in 0m 3s\n",
      "Loss 0.008172613099246981 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 155 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613114029878 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 156 completed in 0m 3s\n",
      "Loss 0.008172612918156475 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 157 completed in 0m 3s\n",
      "Loss 0.00817261312881278 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 158 completed in 0m 3s\n",
      "Loss 0.008172613117725604 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 159 completed in 0m 3s\n",
      "Loss 0.008172613023484624 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 160 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613102942702 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 161 completed in 0m 3s\n",
      "Loss 0.00817261311218202 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 162 completed in 0m 3s\n",
      "Loss 0.008172612958809448 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 163 completed in 0m 3s\n",
      "Loss 0.008172612979135931 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 164 completed in 0m 3s\n",
      "Loss 0.008172613136204222 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 165 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172612999462415 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 166 completed in 0m 3s\n",
      "Loss 0.008172613145443531 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 167 completed in 0m 3s\n",
      "Loss 0.008172612951418003 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 168 completed in 0m 3s\n",
      "Loss 0.008172613110334164 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 169 completed in 0m 3s\n",
      "Loss 0.00817261309555126 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 170 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.00817261303272394 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 171 completed in 0m 3s\n",
      "Loss 0.008172613080768356 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 172 completed in 0m 3s\n",
      "Loss 0.008172613047506832 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 173 completed in 0m 3s\n",
      "Loss 0.008172613091855534 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 174 completed in 0m 3s\n",
      "Loss 0.008172613150987124 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 175 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613147291398 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 176 completed in 0m 3s\n",
      "Loss 0.008172613043811113 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 177 completed in 0m 3s\n",
      "Loss 0.0081726129662009 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 178 completed in 0m 3s\n",
      "Loss 0.008172613106638438 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 179 completed in 0m 3s\n",
      "Loss 0.008172613176857201 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 180 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613132508496 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 181 completed in 0m 3s\n",
      "Loss 0.008172612999462415 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 182 completed in 0m 3s\n",
      "Loss 0.008172613112182012 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 183 completed in 0m 3s\n",
      "Loss 0.00817261307152905 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 184 completed in 0m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.008172613117725597 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 185 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613029028214 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 186 completed in 0m 3s\n",
      "Loss 0.008172613082616223 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 187 completed in 0m 3s\n",
      "Loss 0.008172613073376904 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 188 completed in 0m 3s\n",
      "Loss 0.008172613073376904 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 189 completed in 0m 3s\n",
      "Loss 0.008172613043811111 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 190 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613143595674 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 191 completed in 0m 3s\n",
      "Loss 0.008172612992070971 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 192 completed in 0m 3s\n",
      "Loss 0.008172612997614556 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 193 completed in 0m 3s\n",
      "Loss 0.008172613047506843 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 194 completed in 0m 3s\n",
      "Loss 0.00817261299761456 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 195 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.00817261309924698 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 196 completed in 0m 3s\n",
      "Loss 0.0081726131177256 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 197 completed in 0m 3s\n",
      "Loss 0.008172613051202557 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 198 completed in 0m 3s\n",
      "Loss 0.008172613069681185 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 199 completed in 0m 3s\n",
      "Loss 0.00817261302533249 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 200 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613195335821 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 201 completed in 0m 3s\n",
      "Loss 0.008172612992070966 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 202 completed in 0m 3s\n",
      "Loss 0.008172613062289733 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 203 completed in 0m 3s\n",
      "Loss 0.008172613154682852 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 204 completed in 0m 3s\n",
      "Loss 0.008172613150987124 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 205 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613029028204 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 206 completed in 0m 3s\n",
      "Loss 0.008172612995766701 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 207 completed in 0m 3s\n",
      "Loss 0.008172613006853869 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 208 completed in 0m 3s\n",
      "Loss 0.008172613091855529 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 209 completed in 0m 3s\n",
      "Loss 0.008172613099246981 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 210 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613114029881 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 211 completed in 0m 3s\n",
      "Loss 0.00817261304381111 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 212 completed in 0m 3s\n",
      "Loss 0.008172613036419665 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 213 completed in 0m 3s\n",
      "Loss 0.008172612966200896 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 214 completed in 0m 3s\n",
      "Loss 0.008172613187944365 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 215 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613058594012 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 216 completed in 0m 3s\n",
      "Loss 0.008172613005006006 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 217 completed in 0m 3s\n",
      "Loss 0.008172612995766694 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 218 completed in 0m 3s\n",
      "Loss 0.008172613025332497 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 219 completed in 0m 3s\n",
      "Loss 0.008172613077072637 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 220 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172612992070965 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 221 completed in 0m 3s\n",
      "Loss 0.00817261319903154 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 222 completed in 0m 3s\n",
      "Loss 0.008172613090007672 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 223 completed in 0m 3s\n",
      "Loss 0.008172613036419665 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 224 completed in 0m 3s\n",
      "Loss 0.008172613029028216 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 225 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172612921852205 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 226 completed in 0m 3s\n",
      "Loss 0.008172613043811111 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 227 completed in 0m 3s\n",
      "Loss 0.008172613006853869 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 228 completed in 0m 3s\n",
      "Loss 0.00817261310479056 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 229 completed in 0m 3s\n",
      "Loss 0.008172613038267523 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 230 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613062289743 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 231 completed in 0m 3s\n",
      "Loss 0.008172613187944376 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 232 completed in 0m 3s\n",
      "Loss 0.00817261307337691 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 233 completed in 0m 3s\n",
      "Loss 0.008172612999462413 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 234 completed in 0m 3s\n",
      "Loss 0.008172613060441876 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 235 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613126964911 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 236 completed in 0m 3s\n",
      "Loss 0.008172613017941038 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 237 completed in 0m 3s\n",
      "Loss 0.008172613043811117 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 238 completed in 0m 3s\n",
      "Loss 0.008172613128812777 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 239 completed in 0m 3s\n",
      "Loss 0.00817261312881278 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 240 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613139899946 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 241 completed in 0m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.00817261299576669 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 242 completed in 0m 3s\n",
      "Loss 0.008172613082616223 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 243 completed in 0m 3s\n",
      "Loss 0.008172612966200902 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 244 completed in 0m 3s\n",
      "Loss 0.008172613125117053 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 245 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613108486286 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 246 completed in 0m 3s\n",
      "Loss 0.008172613139899951 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 247 completed in 0m 3s\n",
      "Loss 0.008172613093703393 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 248 completed in 0m 3s\n",
      "Loss 0.008172613069681188 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 249 completed in 0m 3s\n",
      "Loss 0.008172613099246981 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 250 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613184248641 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 251 completed in 0m 3s\n",
      "Loss 0.008172613069681183 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 252 completed in 0m 3s\n",
      "Loss 0.00817261303641967 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 253 completed in 0m 3s\n",
      "Loss 0.00817261294957013 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 254 completed in 0m 3s\n",
      "Loss 0.008172613143595674 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 255 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613106638426 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 256 completed in 0m 3s\n",
      "Loss 0.008172613014245312 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 257 completed in 0m 3s\n",
      "Loss 0.008172613032723937 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 258 completed in 0m 3s\n",
      "Loss 0.008172613160226432 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 259 completed in 0m 3s\n",
      "Loss 0.008172613243380237 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 260 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613036419663 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 261 completed in 0m 3s\n",
      "Loss 0.00817261303272394 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 262 completed in 0m 3s\n",
      "Loss 0.008172613036419666 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 263 completed in 0m 3s\n",
      "Loss 0.008172613235988786 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 264 completed in 0m 3s\n",
      "Loss 0.00817261299576669 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 265 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613093703393 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 266 completed in 0m 3s\n",
      "Loss 0.008172613108486297 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 267 completed in 0m 3s\n",
      "Loss 0.00817261305489829 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 268 completed in 0m 3s\n",
      "Loss 0.008172613064137592 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 269 completed in 0m 3s\n",
      "Loss 0.008172613054898286 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 270 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613088159803 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 271 completed in 0m 3s\n",
      "Loss 0.008172613112182016 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 272 completed in 0m 3s\n",
      "Loss 0.008172613126964918 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 273 completed in 0m 3s\n",
      "Loss 0.00817261306783332 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 274 completed in 0m 3s\n",
      "Loss 0.008172612962505174 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 275 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613099246984 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 276 completed in 0m 3s\n",
      "Loss 0.008172613047506837 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 277 completed in 0m 3s\n",
      "Loss 0.008172612892286412 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 278 completed in 0m 3s\n",
      "Loss 0.008172613029028213 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 279 completed in 0m 3s\n",
      "Loss 0.008172612890438543 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 280 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0081726129902231 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 281 completed in 0m 3s\n",
      "Loss 0.008172613108486285 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 282 completed in 0m 3s\n",
      "Loss 0.008172613043811115 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 283 completed in 0m 3s\n",
      "Loss 0.008172612977288078 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 284 completed in 0m 3s\n",
      "Loss 0.008172612988375245 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 285 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172612951418008 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 286 completed in 0m 3s\n",
      "Loss 0.008172612938482964 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 287 completed in 0m 3s\n",
      "Loss 0.008172613038267534 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 288 completed in 0m 3s\n",
      "Loss 0.008172613053050424 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 289 completed in 0m 3s\n",
      "Loss 0.00817261306598546 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 290 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613010549596 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 291 completed in 0m 3s\n",
      "Loss 0.008172613193487955 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 292 completed in 0m 3s\n",
      "Loss 0.008172612995766698 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 293 completed in 0m 3s\n",
      "Loss 0.008172613030876082 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 294 completed in 0m 3s\n",
      "Loss 0.008172613147291395 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 295 completed in 0m 3s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.008172613114029881 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 296 completed in 0m 3s\n",
      "Loss 0.00817261301609318 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 297 completed in 0m 3s\n",
      "Loss 0.008172612942178685 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 298 completed in 0m 3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.008172612951418001 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 299 completed in 0m 3s\n",
      "Loss 0.008172613058594016 is bigger than Loss 0.008172612707500186 in the prev epoch \n",
      "Loss = 0.0082 at epoch 300 completed in 0m 3s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=1.6000000000000003e-07, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss = float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 300, 5670, 0, 45, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,2)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/corridor_new_data_bgr_3_NOV.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.4f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/corridor_new_data_bgr_600.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([912, 1])\n",
      "num of batches ---> 114\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 8\n",
    "n_examples = 912\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred[start:end] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(912, 1)\n"
     ]
    }
   ],
   "source": [
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([912])\n",
      "MSEloss==0.4225935859552363 ABSloss==0.49924373502532643\n"
     ]
    }
   ],
   "source": [
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0])\n",
    "print(dif.size())\n",
    "np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSEloss = torch.mean(torch.pow(dif,2))\n",
    "ABSloss = torch.mean(dif)\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.9617797666556\n",
      "43.8015182038607\n",
      "-51.160261562794894\n"
     ]
    }
   ],
   "source": [
    "n=887\n",
    "\n",
    "print(finalpred.data[n,0]*(180/np.pi))\n",
    "print(ytestT[n,0]*(180/np.pi))\n",
    "print(ytestT[n,0]*(180/np.pi)-finalpred.data[n,0]*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.604558965298796"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ABSloss*(180/np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  91.1458\n",
      "  88.8542\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  49.5078\n",
      " 130.4922\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  48.2572\n",
      " 131.7428\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      "  45.0000\n",
      " 135.0000\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 133.6780\n",
      "  46.3220\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 129.8479\n",
      "  50.1521\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      " 135.2195\n",
      "  44.7805\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  89.2310\n",
      "  90.7690\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      "  90.0000\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 128.3278\n",
      "  51.6722\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 130.2191\n",
      "  49.7809\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      " 129.6904\n",
      "  50.3096\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  48.8410\n",
      " 131.1591\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  47.4717\n",
      " 132.5283\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  46.1853\n",
      " 133.8147\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  88.7897\n",
      "  91.2103\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      "  89.5659\n",
      "  90.4341\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 131.0091\n",
      "  48.9909\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 132.4362\n",
      "  47.5638\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      " 133.4982\n",
      "  46.5018\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  48.7266\n",
      " 131.2734\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  51.2345\n",
      " 128.7655\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  50.1104\n",
      " 129.8896\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  88.8693\n",
      "  91.1307\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  42.8202\n",
      " 137.1798\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  43.0251\n",
      " 136.9749\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      "  39.7718\n",
      " 140.2282\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 137.8624\n",
      "  42.1376\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 136.3220\n",
      "  43.6780\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      " 138.3419\n",
      "  41.6581\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  89.2838\n",
      "  90.7162\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  88.8465\n",
      "  91.1535\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.5233\n",
      " 132.4767\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  47.0166\n",
      " 132.9834\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      "  42.1902\n",
      " 137.8098\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.8840\n",
      "  43.1159\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 136.1985\n",
      "  43.8015\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      " 137.6367\n",
      "  42.3632\n",
      "[torch.FloatTensor of size 912]\n",
      "\n",
      "torch.Size([912])\n"
     ]
    }
   ],
   "source": [
    "#print(ytestT[:,0]*(180/np.pi))\n",
    "print(ytestT[:,0]*(180/np.pi))\n",
    "a = ytestT[:,0]*(180/np.pi)\n",
    "print(a.size())\n",
    "np.savetxt(\"test.csv\", a.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 173.5292\n",
      " 224.8487\n",
      " 282.5188\n",
      " 107.4965\n",
      "  98.2198\n",
      " 105.7313\n",
      "  89.3513\n",
      " 101.4670\n",
      "  81.5013\n",
      "  94.8344\n",
      "  87.1014\n",
      "  86.9526\n",
      "  96.5917\n",
      "  82.2452\n",
      "  90.9523\n",
      "  76.5235\n",
      "  85.9288\n",
      "  74.4137\n",
      "  79.8457\n",
      "  68.9614\n",
      "  73.6388\n",
      "  72.2270\n",
      "  68.8498\n",
      " 109.9282\n",
      "  48.3227\n",
      " 141.1301\n",
      "  52.6376\n",
      " 129.2402\n",
      "  48.7278\n",
      " 124.6510\n",
      "  45.8315\n",
      " 124.2019\n",
      "  56.5810\n",
      " 120.8452\n",
      "  56.7226\n",
      " 110.9932\n",
      "  54.9241\n",
      " 128.3802\n",
      "  67.4745\n",
      " 150.6300\n",
      "  68.0510\n",
      " 140.2196\n",
      "  66.3954\n",
      " 131.0873\n",
      "  49.7283\n",
      " 123.9580\n",
      "  58.4041\n",
      " 138.0071\n",
      "  37.4412\n",
      " 135.9435\n",
      "  44.2135\n",
      " 137.0186\n",
      "  49.5745\n",
      " 126.6363\n",
      "  42.3295\n",
      " 125.9399\n",
      "  48.1420\n",
      " 149.1017\n",
      "  45.8872\n",
      " 154.6732\n",
      "  31.8942\n",
      " 140.1601\n",
      "  47.6222\n",
      " 150.1557\n",
      "  75.6302\n",
      " 144.4570\n",
      "  73.6798\n",
      " 122.6927\n",
      "  74.9949\n",
      " 138.5825\n",
      "  79.7916\n",
      " 120.4884\n",
      " 119.4312\n",
      "  74.2788\n",
      " 135.4987\n",
      "  54.1329\n",
      "  98.6279\n",
      "  55.7520\n",
      "  99.7668\n",
      "  83.5860\n",
      "  86.2377\n",
      "  91.8369\n",
      "  86.9140\n",
      "  92.9795\n",
      "  84.8558\n",
      "  81.5958\n",
      "  89.3448\n",
      " 108.6319\n",
      "  82.8587\n",
      "  73.4166\n",
      "  80.8266\n",
      "  97.5872\n",
      "  97.0835\n",
      "  82.3764\n",
      "  83.2804\n",
      "  73.5336\n",
      " 162.1739\n",
      "  47.3875\n",
      " 164.0884\n",
      "  49.0205\n",
      " 149.6523\n",
      "  54.0031\n",
      " 160.5559\n",
      "  48.7603\n",
      " 145.8795\n",
      "  41.3270\n",
      " 121.5684\n",
      "  44.3158\n",
      " 117.7829\n",
      "  48.2011\n",
      " 113.2194\n",
      "  56.9580\n",
      " 122.4556\n",
      "  55.5060\n",
      " 127.4697\n",
      "  63.4079\n",
      " 112.2420\n",
      "  77.5000\n",
      " 136.2745\n",
      " 106.7625\n",
      " 166.0653\n",
      "  53.8879\n",
      " 167.2502\n",
      "  54.6221\n",
      " 168.2817\n",
      "  52.6362\n",
      " 141.1230\n",
      "  70.0443\n",
      " 131.6858\n",
      "  87.9540\n",
      " 122.8642\n",
      "  68.3451\n",
      " 114.5580\n",
      "  62.6840\n",
      " 103.7269\n",
      "  61.2016\n",
      "  75.1421\n",
      "  59.4724\n",
      "  89.3352\n",
      "  35.1282\n",
      "  86.2163\n",
      "  42.8516\n",
      "  65.7859\n",
      "  43.6400\n",
      " 108.7482\n",
      "  44.9716\n",
      " 118.6572\n",
      "  49.9978\n",
      " 121.3626\n",
      "  61.0272\n",
      " 131.3750\n",
      "  74.1532\n",
      " 127.9662\n",
      "  78.8811\n",
      " 149.6275\n",
      "  72.5707\n",
      " 122.3690\n",
      "  57.7791\n",
      "  92.0480\n",
      "  55.1028\n",
      "  60.3791\n",
      "  84.4705\n",
      "  71.1049\n",
      "  80.3987\n",
      "  62.0397\n",
      "  45.0206\n",
      "  64.5667\n",
      "  58.6282\n",
      "  66.3305\n",
      " 101.8804\n",
      "  57.5676\n",
      "  99.2749\n",
      "  65.0819\n",
      " 105.5235\n",
      "  86.3515\n",
      "  91.6199\n",
      " 117.4712\n",
      "  72.8748\n",
      "  96.0780\n",
      "  73.3222\n",
      " 101.2587\n",
      "  69.6681\n",
      " 112.7723\n",
      "  52.9299\n",
      " 108.2646\n",
      "  53.6424\n",
      "  95.3893\n",
      "  80.2300\n",
      "  85.0060\n",
      " 101.8073\n",
      " 103.2631\n",
      "  83.2098\n",
      "  75.3196\n",
      "  82.1772\n",
      "  72.0317\n",
      "  96.3615\n",
      "  57.8513\n",
      " 126.2362\n",
      "  69.2234\n",
      " 103.5441\n",
      "  38.5688\n",
      " 101.2445\n",
      "  44.3622\n",
      " 109.7500\n",
      "  39.9220\n",
      " 106.4680\n",
      "  33.1063\n",
      " 119.1127\n",
      "  36.3080\n",
      " 155.6333\n",
      "  51.5696\n",
      " 141.3985\n",
      "  66.7279\n",
      " 110.8168\n",
      " 101.9622\n",
      " 108.8620\n",
      " 129.6417\n",
      "  69.8715\n",
      " 126.1672\n",
      "  78.9742\n",
      " 120.2385\n",
      "  83.6362\n",
      " 129.2240\n",
      "  81.7106\n",
      " 125.8072\n",
      "  86.5039\n",
      "  96.3765\n",
      "  87.5557\n",
      "  72.5025\n",
      "  82.7922\n",
      "  58.3222\n",
      "  88.9329\n",
      "  59.7913\n",
      " 102.0822\n",
      "  68.0501\n",
      "  96.3529\n",
      "  71.2494\n",
      " 100.4360\n",
      "  75.4749\n",
      " 103.4982\n",
      " 144.3990\n",
      "  54.6478\n",
      " 154.7944\n",
      "  49.5217\n",
      " 115.1974\n",
      "  48.9183\n",
      " 102.5149\n",
      "  52.6959\n",
      " 103.9197\n",
      "  77.9517\n",
      " 115.5833\n",
      "  73.7928\n",
      "  95.1797\n",
      "  79.2173\n",
      "  98.1666\n",
      "  82.4262\n",
      " 106.1476\n",
      "  79.1212\n",
      " 123.0077\n",
      "  95.0936\n",
      " 100.2925\n",
      " 116.4306\n",
      " 117.9832\n",
      " 127.8140\n",
      "  79.0357\n",
      " 117.5845\n",
      "  69.4315\n",
      " 113.8977\n",
      "  74.2444\n",
      " 109.1113\n",
      "  85.0018\n",
      " 110.5944\n",
      "  89.3821\n",
      " 101.1539\n",
      "  75.0390\n",
      " 109.6885\n",
      "  92.3768\n",
      " 107.2361\n",
      " 119.5368\n",
      "  73.9171\n",
      " 111.0544\n",
      "  57.7389\n",
      " 129.5815\n",
      "  53.3662\n",
      " 138.8317\n",
      "  68.2264\n",
      " 108.3554\n",
      "  56.7809\n",
      "  76.3279\n",
      "  76.1549\n",
      "  76.9130\n",
      "  79.7983\n",
      "  76.4619\n",
      "  77.2151\n",
      "  68.5898\n",
      "  77.6872\n",
      "  64.1638\n",
      "  80.5277\n",
      "  78.3280\n",
      "  63.2906\n",
      "  80.7182\n",
      "  68.4493\n",
      "  81.0492\n",
      "  74.1981\n",
      "  79.5481\n",
      "  77.2139\n",
      "  84.9916\n",
      " 104.6380\n",
      "  66.4422\n",
      " 113.4325\n",
      "  67.0987\n",
      " 123.1727\n",
      "  31.5962\n",
      " 159.6696\n",
      "  31.9716\n",
      " 165.7433\n",
      "  35.3548\n",
      " 174.8372\n",
      "  40.2818\n",
      " 168.5950\n",
      "  59.3681\n",
      " 171.5603\n",
      "  52.3929\n",
      " 148.1279\n",
      "  54.8868\n",
      " 120.1748\n",
      "  73.7096\n",
      " 114.2858\n",
      "  70.5701\n",
      " 114.0137\n",
      "  73.0798\n",
      " 106.0478\n",
      "  70.2418\n",
      " 117.6062\n",
      "  73.8540\n",
      " 120.4003\n",
      "  90.1448\n",
      "  93.8716\n",
      "  84.1579\n",
      "  86.5812\n",
      "  80.9902\n",
      " 119.9638\n",
      " 103.9854\n",
      " 147.8037\n",
      " 104.2166\n",
      " 152.1011\n",
      " 118.5263\n",
      " 147.0605\n",
      " 109.1271\n",
      " 126.7762\n",
      "  79.6725\n",
      "  92.3847\n",
      "  64.5820\n",
      " 121.9036\n",
      "  57.7576\n",
      " 121.1074\n",
      "  59.9550\n",
      " 152.6881\n",
      "  51.6623\n",
      " 157.8693\n",
      " 110.4733\n",
      "  84.3078\n",
      " 103.5707\n",
      "  73.7062\n",
      "  99.2704\n",
      "  85.6861\n",
      " 101.4392\n",
      " 106.6589\n",
      " 110.4239\n",
      " 105.4150\n",
      " 125.3903\n",
      "  85.7247\n",
      " 122.5925\n",
      "  74.7169\n",
      " 110.1401\n",
      "  73.5048\n",
      "  89.8125\n",
      "  67.9967\n",
      "  84.4620\n",
      "  64.8960\n",
      "  92.2506\n",
      "  65.2150\n",
      " 112.9975\n",
      "  88.2283\n",
      "  57.6451\n",
      " 148.1473\n",
      "  48.4649\n",
      " 132.1382\n",
      "  53.2914\n",
      " 114.0849\n",
      "  60.3485\n",
      " 120.2794\n",
      "  54.2724\n",
      "  94.3134\n",
      "  68.2779\n",
      " 100.6237\n",
      "  69.9439\n",
      "  97.9747\n",
      "  54.0372\n",
      " 120.4356\n",
      "  85.2419\n",
      " 130.4310\n",
      "  84.7138\n",
      " 122.4012\n",
      "  52.0424\n",
      " 126.4756\n",
      "  73.3921\n",
      " 100.4383\n",
      " 105.2596\n",
      "  83.1720\n",
      " 109.1825\n",
      "  82.6043\n",
      " 104.9774\n",
      "  99.0192\n",
      "  96.2811\n",
      " 115.0724\n",
      "  97.4265\n",
      " 126.1192\n",
      " 107.2519\n",
      " 120.2521\n",
      " 137.5351\n",
      " 112.4612\n",
      " 143.3444\n",
      "  87.9295\n",
      " 137.2720\n",
      "  52.5223\n",
      " 135.9710\n",
      "  42.5310\n",
      " 146.0441\n",
      "  42.5757\n",
      " 136.5956\n",
      "  51.1259\n",
      " 100.5158\n",
      "  90.5015\n",
      "  98.7696\n",
      "  93.1348\n",
      "  96.2214\n",
      " 104.0231\n",
      "  89.4238\n",
      " 105.1652\n",
      "  92.5326\n",
      " 115.5817\n",
      "  85.2098\n",
      " 125.0153\n",
      "  92.8800\n",
      " 129.6192\n",
      " 120.5370\n",
      " 100.6665\n",
      " 131.3136\n",
      "  92.2495\n",
      " 148.3830\n",
      "  77.0208\n",
      " 141.7059\n",
      "  56.4400\n",
      " 134.8423\n",
      "  53.5121\n",
      " 174.5951\n",
      "  59.7967\n",
      " 152.9313\n",
      "  62.9563\n",
      " 131.1005\n",
      "  58.4088\n",
      "  98.6248\n",
      "  45.3874\n",
      " 114.1278\n",
      "  33.6319\n",
      " 100.2477\n",
      "  43.4334\n",
      " 113.4801\n",
      "  53.8594\n",
      "  83.3068\n",
      "  38.0809\n",
      " 156.8523\n",
      "  37.9870\n",
      " 149.2041\n",
      "  50.5448\n",
      " 122.0655\n",
      "  28.3126\n",
      " 112.4529\n",
      "  51.5804\n",
      "  53.6295\n",
      " 121.1649\n",
      "  64.5241\n",
      " 110.8150\n",
      "  77.0406\n",
      "  93.8237\n",
      "  96.1160\n",
      "  86.6450\n",
      "  98.3145\n",
      "  90.3287\n",
      "  86.6591\n",
      " 106.3495\n",
      "  89.4858\n",
      " 104.2224\n",
      "  98.5475\n",
      " 118.1171\n",
      "  97.0278\n",
      " 100.0215\n",
      "  99.2195\n",
      " 126.9308\n",
      " 115.5904\n",
      "  96.3856\n",
      " 114.2212\n",
      "  61.4931\n",
      " 107.0349\n",
      " 113.4905\n",
      " 116.5374\n",
      " 115.5146\n",
      "  97.2212\n",
      " 117.5740\n",
      "  76.6782\n",
      " 120.3705\n",
      "  84.8605\n",
      " 126.7222\n",
      "  92.6315\n",
      " 147.7967\n",
      "  92.7481\n",
      " 153.7701\n",
      "  77.1226\n",
      " 151.9348\n",
      "  77.7489\n",
      " 148.2366\n",
      "  59.5736\n",
      " 133.5581\n",
      "  79.1766\n",
      " 114.5709\n",
      "  73.4186\n",
      " 104.9304\n",
      "  37.3626\n",
      " 150.7081\n",
      "  28.7444\n",
      " 155.2910\n",
      "  32.9610\n",
      " 147.1267\n",
      "  39.8534\n",
      " 135.6128\n",
      "  38.0933\n",
      " 143.4003\n",
      "  42.3483\n",
      " 155.4840\n",
      "  62.5639\n",
      " 164.5462\n",
      "  63.5501\n",
      " 136.4621\n",
      "  71.5547\n",
      " 120.9115\n",
      "  77.1532\n",
      "  96.8446\n",
      " 107.4264\n",
      "  67.7299\n",
      " 115.8613\n",
      "  54.0583\n",
      "  94.1742\n",
      " 104.5597\n",
      "  96.7517\n",
      " 122.3337\n",
      "  93.4097\n",
      " 116.3943\n",
      " 104.4121\n",
      " 109.1883\n",
      " 114.7982\n",
      " 105.3927\n",
      " 123.1815\n",
      " 100.0276\n",
      " 134.1742\n",
      "  80.0133\n",
      " 116.0755\n",
      "  80.1317\n",
      "  95.0649\n",
      "  59.9072\n",
      "  87.8716\n",
      "  73.7327\n",
      "  68.5431\n",
      "  90.4097\n",
      "  61.4094\n",
      " 100.2154\n",
      "  76.4062\n",
      "  81.1553\n",
      "  86.2927\n",
      "  77.2162\n",
      "  98.5936\n",
      "  80.1058\n",
      " 106.6284\n",
      "  72.2763\n",
      " 104.9733\n",
      "  59.6222\n",
      " 107.2184\n",
      "  50.9740\n",
      " 124.6427\n",
      "  55.4515\n",
      " 136.3758\n",
      "  49.1758\n",
      " 109.1154\n",
      "  53.7868\n",
      "  97.6309\n",
      "  72.5037\n",
      "  67.2655\n",
      "  85.1130\n",
      "  48.4703\n",
      "  82.0402\n",
      "  49.1477\n",
      " 123.8958\n",
      "  56.0408\n",
      " 105.9471\n",
      "  71.8160\n",
      "  98.6855\n",
      "  79.8312\n",
      "  85.7062\n",
      "  86.2673\n",
      "  68.5503\n",
      "  95.5409\n",
      "  67.6243\n",
      " 107.5981\n",
      "  56.9181\n",
      " 128.6651\n",
      "  53.2757\n",
      " 136.1624\n",
      "  29.0290\n",
      " 164.8883\n",
      "  14.3284\n",
      " 157.3528\n",
      "  32.8137\n",
      " 142.3332\n",
      "  43.0406\n",
      "  75.9167\n",
      " 131.7923\n",
      "  59.5584\n",
      " 129.9388\n",
      "  45.8753\n",
      " 130.9920\n",
      "  25.6832\n",
      " 130.5283\n",
      "  27.8630\n",
      " 127.5383\n",
      "  29.2553\n",
      " 139.3857\n",
      "  33.9188\n",
      " 163.4631\n",
      "  33.6864\n",
      " 169.9353\n",
      "  30.0374\n",
      " 153.6471\n",
      "  12.6823\n",
      " 164.9409\n",
      "  21.1373\n",
      " 165.2524\n",
      "  17.2322\n",
      " 113.1084\n",
      " 134.2250\n",
      "  65.3469\n",
      " 127.0546\n",
      "  55.6639\n",
      " 125.1278\n",
      "  47.6385\n",
      " 124.5512\n",
      "  59.2294\n",
      " 110.6550\n",
      "  56.9384\n",
      " 117.1940\n",
      "  51.6760\n",
      " 130.6544\n",
      "  53.1582\n",
      " 112.8581\n",
      "  56.7467\n",
      "  71.0574\n",
      "  46.5604\n",
      "  69.6723\n",
      "  70.0402\n",
      "  71.0452\n",
      "  83.7652\n",
      "  73.4065\n",
      "  60.7870\n",
      "  86.1429\n",
      " 131.6980\n",
      "  78.4160\n",
      " 117.6938\n",
      "  77.4277\n",
      "  91.7058\n",
      "  79.9328\n",
      " 107.6166\n",
      "  68.5051\n",
      "  69.3925\n",
      "  72.4964\n",
      "  57.0579\n",
      "  61.8978\n",
      "  69.3917\n",
      "  57.1068\n",
      "  66.5528\n",
      "  62.6440\n",
      "  52.4035\n",
      "  82.9035\n",
      "  59.2825\n",
      "  97.2934\n",
      "  87.5773\n",
      "  89.6999\n",
      "  93.5768\n",
      "  92.4657\n",
      "  78.5807\n",
      "  91.1302\n",
      "  89.0825\n",
      "  97.0236\n",
      "  81.9308\n",
      "  94.4804\n",
      "  79.3789\n",
      "  95.5211\n",
      "  84.1978\n",
      "  93.4890\n",
      " 101.3315\n",
      "  85.4080\n",
      "  94.7493\n",
      "  92.0552\n",
      "  80.8275\n",
      " 118.9032\n",
      "  67.8507\n",
      " 140.2290\n",
      "  49.5628\n",
      " 146.3132\n",
      "  43.8813\n",
      " 158.4344\n",
      "  40.2540\n",
      "  46.5812\n",
      " 132.5450\n",
      "  37.8698\n",
      " 148.7224\n",
      "  44.7316\n",
      " 137.1945\n",
      "  60.3908\n",
      " 114.2705\n",
      "  48.8321\n",
      "  84.3333\n",
      "  62.1167\n",
      "  89.0535\n",
      "  56.5297\n",
      "  96.2679\n",
      "  95.1269\n",
      " 129.3462\n",
      "  95.0296\n",
      " 101.6321\n",
      " 100.3538\n",
      "  96.2997\n",
      " 116.8362\n",
      "  69.9094\n",
      "  85.5651\n",
      "  61.7060\n",
      "  70.6322\n",
      "  93.6764\n",
      "  69.9456\n",
      "  92.5073\n",
      "  75.3750\n",
      "  87.7694\n",
      "  78.3900\n",
      "  98.2361\n",
      "  72.6572\n",
      " 101.9685\n",
      "  78.2122\n",
      "  97.0639\n",
      "  90.0599\n",
      "  98.2791\n",
      "  82.1137\n",
      " 129.2013\n",
      "  78.1526\n",
      " 109.2473\n",
      "  82.6983\n",
      "  78.5603\n",
      "  88.4513\n",
      "  78.0708\n",
      " 110.2754\n",
      "  78.6233\n",
      "  51.8198\n",
      "  82.2386\n",
      "  49.3700\n",
      "  84.6618\n",
      "  58.6032\n",
      "  92.9760\n",
      "  63.8819\n",
      "  98.8657\n",
      "  62.1750\n",
      " 107.5317\n",
      "  63.5156\n",
      " 105.7045\n",
      "  67.9835\n",
      "  92.2847\n",
      "  68.0874\n",
      "  89.2965\n",
      "  63.7472\n",
      "  93.2541\n",
      "  57.8037\n",
      " 102.2353\n",
      "  54.7425\n",
      " 141.0626\n",
      "  60.7302\n",
      " 156.3931\n",
      "  46.2386\n",
      " 127.2085\n",
      "  46.1458\n",
      " 131.7705\n",
      "  46.5966\n",
      " 133.9176\n",
      "  46.8764\n",
      " 136.4731\n",
      "  45.2481\n",
      " 132.2308\n",
      "  46.4996\n",
      " 133.2555\n",
      "  49.7810\n",
      " 138.2623\n",
      "  48.2666\n",
      " 133.3788\n",
      "  50.7227\n",
      " 118.7587\n",
      "  50.0327\n",
      " 120.0249\n",
      "  54.9148\n",
      " 115.4683\n",
      "  59.3913\n",
      " 154.4933\n",
      "  92.3150\n",
      "  71.8947\n",
      " 100.8070\n",
      "  71.1333\n",
      "  83.8554\n",
      "  71.3723\n",
      "  74.6282\n",
      "  77.2380\n",
      "  72.6742\n",
      "  83.6885\n",
      "  70.4062\n",
      "  69.2247\n",
      "  50.8312\n",
      "  77.6860\n",
      "  44.0300\n",
      "  92.0368\n",
      "  56.8405\n",
      " 112.1872\n",
      "  72.5855\n",
      " 132.4608\n",
      "  78.6937\n",
      " 127.8741\n",
      "  47.6291\n",
      " 179.6500\n",
      "  95.7264\n",
      "  98.0165\n",
      " 112.1028\n",
      " 108.2748\n",
      " 111.0967\n",
      " 100.0582\n",
      " 134.2977\n",
      " 103.4123\n",
      " 115.3747\n",
      "  88.5876\n",
      " 114.7453\n",
      "  92.4850\n",
      "  93.0059\n",
      "  82.9808\n",
      "  97.8985\n",
      "  66.4727\n",
      "  85.2200\n",
      "  59.5864\n",
      "  83.1474\n",
      "  71.9547\n",
      " 100.8628\n",
      "  87.7427\n",
      " 103.0795\n",
      " 104.9406\n",
      "  97.9536\n",
      "  97.4658\n",
      "  87.6825\n",
      "  91.3932\n",
      "  81.6919\n",
      "  87.4660\n",
      "  75.4803\n",
      " 100.3216\n",
      "  67.0776\n",
      " 111.9926\n",
      "  56.0365\n",
      "  99.9453\n",
      "  56.7099\n",
      " 100.7334\n",
      "  47.8318\n",
      " 117.4256\n",
      "  41.8874\n",
      " 119.7599\n",
      "  55.1411\n",
      " 118.8966\n",
      "  54.5988\n",
      " 113.2942\n",
      "  64.6015\n",
      "  94.9618\n",
      " 163.9736\n",
      "  43.8510\n",
      " 149.7222\n",
      "  49.5671\n",
      " 135.4985\n",
      "  39.0351\n",
      " 140.8813\n",
      "  36.9464\n",
      " 133.3692\n",
      "  37.1413\n",
      " 141.7799\n",
      "  38.7761\n",
      " 127.0161\n",
      "  39.5661\n",
      " 136.8188\n",
      "  38.5704\n",
      " 139.7553\n",
      "  29.5101\n",
      " 134.6861\n",
      "  36.3635\n",
      " 147.2310\n",
      "  38.0770\n",
      " 137.0087\n",
      "  47.8266\n",
      "[torch.FloatTensor of size 912]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(finalpred.data[:,0]*(180/np.pi))\n",
    "print(finalpred.data[:,0]*(180/np.pi))\n",
    "b = finalpred.data[:,0]*(180/np.pi)\n",
    "c=torch.abs(ytestT[:,0]*(180/np.pi)- finalpred.data[:,0]*(180/np.pi))\n",
    "np.savetxt(\"pred.csv\", b.numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"diff.csv\", c.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4546151161193848"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalpred.data[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008, 736, 3)\n",
      "Angle===58.387817478340644 1.019059658050537\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAACiCAYAAACDBmukAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7658000f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "test = cv2.imread(\"./Test_Net_image/4.JPG\")\n",
    "print(test.shape)\n",
    "test = imresize(test,(180,320,3))\n",
    "#imshow(test)\n",
    "test = test.transpose(2,0,1)\n",
    "test = np.reshape(test,(1,3,180,320))\n",
    "test = test.astype(np.float32)\n",
    "testPT = torch.from_numpy(test).float()\n",
    "testPT = batch_rgb_to_bgr(testPT)\n",
    "testPT = torch.div(testPT,255.0)\n",
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "testPT = norm(testPT)\n",
    "'''\n",
    "ind = 2000\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0]*(180/np.pi)), ytestT[ind,0])\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "#'''\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Angle===\"+str(test_pred.data[0,0]*(180/np.pi)), test_pred.data[0,0])\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
